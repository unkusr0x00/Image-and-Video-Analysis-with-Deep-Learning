{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Imports",
   "id": "32d736dd469ee957"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import logging\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from pymongo import MongoClient\n",
    "from scenedetect import VideoManager, SceneManager\n",
    "from scenedetect.detectors import ContentDetector, ThresholdDetector\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input as preprocess_input_resnet\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input as preprocess_input_vgg\n",
    "from tqdm import tqdm\n",
    "\n"
   ],
   "id": "3918caaa9649f9c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Video Pre-processing",
   "id": "e22e91d1016abf2e"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Configuration\n",
    "input_dir = 'V3C1-100/'\n",
    "output_dir = 'preprocessed_videos/'\n",
    "output_format = 'mp4'\n",
    "\n",
    "# Configuration Google Colab\n",
    "# input_dir = '/content/drive/MyDrive/V3C1-100'\n",
    "# output_dir = '/content/drive/MyDrive/preprocessed_videos'\n",
    "# output_format = 'mp4'\n",
    "\n",
    "resize_width = 640\n",
    "resize_height = 480\n",
    "convert_to_grayscale = False\n",
    "frame_rate = 24  # Target frame rate\n",
    "max_workers = 1\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def preprocess_video(input_path, output_path, resize_dim, grayscale, frame_rate):\n",
    "    try:\n",
    "        print(f\"Processing video: {input_path}\")\n",
    "        cap = cv2.VideoCapture(input_path)\n",
    "        if not cap.isOpened():\n",
    "            raise ValueError(f\"Failed to open video file: {input_path}\")\n",
    "\n",
    "        original_frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "        if original_frame_rate == 0:\n",
    "            raise ValueError(f\"Failed to get frame rate for video file: {input_path}\")\n",
    "\n",
    "        frame_interval = int(original_frame_rate // frame_rate)\n",
    "\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(output_path, fourcc, frame_rate, resize_dim, not grayscale)\n",
    "\n",
    "        frame_count = 0\n",
    "        prev_gray = None\n",
    "\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            if frame_count % frame_interval == 0:\n",
    "                # Resize frame\n",
    "                frame = cv2.resize(frame, resize_dim, interpolation=cv2.INTER_AREA)\n",
    "                # # Convert to grayscale if needed for optical flow\n",
    "                # gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "                # # Apply noise reduction (computationally expensive)\n",
    "                # frame = cv2.fastNlMeansDenoisingColored(frame, None, 10, 10, 7, 21)\n",
    "                # # Apply histogram equalization\n",
    "                # if grayscale:\n",
    "                #     frame = cv2.equalizeHist(frame)\n",
    "                # else:\n",
    "                #     for i in range(3):\n",
    "                #         frame[:, :, i] = cv2.equalizeHist(frame[:, :, i])\n",
    "                # # Edge detection\n",
    "                # edges = cv2.Canny(frame, 100, 200)\n",
    "\n",
    "                # # Optical flow calculation\n",
    "                # if prev_gray is not None:\n",
    "                #     flow = cv2.calcOpticalFlowFarneback(prev_gray, gray_frame, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "                #     mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "                #     hsv = np.zeros_like(frame)\n",
    "                #     hsv[..., 1] = 255\n",
    "                #     hsv[..., 0] = ang * 180 / np.pi / 2\n",
    "                #     hsv[..., 2] = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX)\n",
    "                #     optical_flow = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "                #     frame = cv2.addWeighted(frame, 0.5, optical_flow, 0.5, 0)\n",
    "                # prev_gray = gray_frame\n",
    "\n",
    "                # # Feature extraction using HOG\n",
    "                # hog = cv2.HOGDescriptor()\n",
    "                # hog_features = hog.compute(frame)\n",
    "\n",
    "                # # Keypoint descriptors (ORB example)\n",
    "                # orb = cv2.ORB_create()\n",
    "                # kp, des = orb.detectAndCompute(frame, None)\n",
    "                # frame = cv2.drawKeypoints(frame, kp, None, color=(0, 255, 0), flags=0)\n",
    "                \n",
    "                out.write(frame)\n",
    "            frame_count += 1\n",
    "\n",
    "        cap.release()\n",
    "        out.release()\n",
    "        print(f\"Successfully processed video: {input_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {input_path}: {e}\")\n",
    "\n",
    "def get_video_files(input_directory):\n",
    "    video_files = []\n",
    "    for root, _, files in os.walk(input_directory):\n",
    "        for file in files:\n",
    "            if file.endswith(('.mp4', '.avi', '.mov', '.mkv')):\n",
    "                video_files.append(os.path.join(root, file))\n",
    "    return video_files\n",
    "\n",
    "def process_videos(video_files, output_directory, resize_dim, grayscale, frame_rate):\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = []\n",
    "        with tqdm(total=len(video_files), desc=\"Processing Videos\", unit=\"video\") as pbar:\n",
    "            for video_file in video_files:\n",
    "                relative_path = os.path.relpath(video_file, input_dir)\n",
    "                output_file = os.path.join(output_directory, os.path.splitext(relative_path)[0] + '.' + output_format)\n",
    "                os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "                future = executor.submit(preprocess_video, video_file, output_file, resize_dim, grayscale, frame_rate)\n",
    "                futures.append(future)\n",
    "\n",
    "            for future in futures:\n",
    "                future.add_done_callback(lambda p: pbar.update())\n",
    "            for future in futures:\n",
    "                future.result()  # Wait for all threads to complete\n",
    "\n",
    "print(\"Starting video pre-processing...\")\n",
    "video_files = get_video_files(input_dir)\n",
    "print(f\"Found {len(video_files)} video files.\")\n",
    "resize_dim = (resize_width, resize_height)\n",
    "process_videos(video_files, output_dir, resize_dim, convert_to_grayscale, frame_rate)\n",
    "print(\"All videos processed successfully.\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Shot Boundary Detection",
   "id": "f756ce17872689ef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Configuration\n",
    "input_dir = 'preprocessed_videos/'\n",
    "output_dir = 'shot_boundaries/'\n",
    "keyframe_dir = 'keyframes/'\n",
    "\n",
    "# Configuration Google Colab\n",
    "# input_dir = '/content/drive/MyDrive/preprocessed_videos'\n",
    "# output_dir = '/content/drive/MyDrive/shot_boundaries'\n",
    "# keyframe_dir = '/content/drive/MyDrive/keyframes'\n",
    "\n",
    "min_scene_length = 15  # Minimum length of a scene in frames\n",
    "threshold = 30.0  # Threshold for the ThresholdDetector\n",
    "min_scene_len = 2  # Minimum number of frames a scene should last\n",
    "hist_threshold = 0.4  # Threshold for histogram comparison\n",
    "\n",
    "# Ensure the output and keyframe directories exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(keyframe_dir, exist_ok=True)\n",
    "\n",
    "def calculate_histogram_difference(frame1, frame2):\n",
    "    hist1 = cv2.calcHist([frame1], [0], None, [256], [0, 256])\n",
    "    hist2 = cv2.calcHist([frame2], [0], None, [256], [0, 256])\n",
    "    cv2.normalize(hist1, hist1)\n",
    "    cv2.normalize(hist2, hist2)\n",
    "    return cv2.compareHist(hist1, hist2, cv2.HISTCMP_CORREL)\n",
    "\n",
    "def detect_shot_boundaries(video_path, output_path, keyframe_path):\n",
    "    video_manager = VideoManager([video_path])\n",
    "    scene_manager = SceneManager()\n",
    "\n",
    "    # Add ContentDetector and ThresholdDetector\n",
    "    scene_manager.add_detector(ContentDetector(threshold=30.0, min_scene_len=min_scene_length))\n",
    "    scene_manager.add_detector(ThresholdDetector(threshold=threshold, min_scene_len=min_scene_len))\n",
    "\n",
    "    video_manager.set_downscale_factor()\n",
    "    video_manager.start()\n",
    "    scene_manager.detect_scenes(frame_source=video_manager)\n",
    "    scenes = scene_manager.get_scene_list()\n",
    "    print(f\"Detected {len(scenes)} scenes in video {video_path}\")\n",
    "\n",
    "    # Additional processing for gradual transitions\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    prev_frame = None\n",
    "    prev_gray = None\n",
    "    frame_num = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        if prev_frame is not None:\n",
    "            hist_diff = calculate_histogram_difference(prev_frame, frame)\n",
    "            if hist_diff < hist_threshold:\n",
    "                # Gradual transition detected\n",
    "                scenes.append((frame_num, frame_num + min_scene_len))\n",
    "            # Motion analysis using optical flow\n",
    "            if prev_gray is not None:\n",
    "                flow = cv2.calcOpticalFlowFarneback(prev_gray, gray_frame, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "                mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "                motion_magnitude = np.mean(mag)\n",
    "                if motion_magnitude > threshold:\n",
    "                    scenes.append((frame_num, frame_num + min_scene_len))\n",
    "        prev_frame = frame\n",
    "        prev_gray = gray_frame\n",
    "        frame_num += 1\n",
    "    cap.release()\n",
    "\n",
    "    # Remove duplicate and sort scenes\n",
    "    scenes = sorted(list(set(scenes)))\n",
    "    print(f\"Total scenes after processing: {len(scenes)}\")\n",
    "\n",
    "    # Save shot boundaries to a file\n",
    "    with open(output_path, 'w') as f:\n",
    "        for start_time, end_time in scenes:\n",
    "            f.write(f\"{start_time.get_timecode()}, {end_time.get_timecode()}\\n\")\n",
    "            # f.write(f\"{start_time.get_frames()}, {end_time.get_frames()}\\n\")\n",
    "        print(f\"Shot boundaries saved to {output_path}\")\n",
    "\n",
    "    # Extract keyframes for each detected scene\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    for start, end in scenes:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, int(start))\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            keyframe_filename = os.path.join(keyframe_path, f\"{os.path.basename(video_path)}_frame_{start.get_timecode()}.jpg\")\n",
    "            cv2.imwrite(keyframe_filename, frame)\n",
    "    cap.release()\n",
    "    print(f\"Keyframes saved to {keyframe_path}\")\n",
    "\n",
    "\n",
    "def process_videos(video_files, output_directory, keyframe_directory):\n",
    "    for video_file in tqdm(video_files, desc=\"Detecting Shot Boundaries\", unit=\"video\"):\n",
    "        output_file = os.path.join(output_directory, os.path.splitext(os.path.basename(video_file))[0] + '_shots.txt')\n",
    "        keyframe_path = os.path.join(keyframe_directory, os.path.splitext(os.path.basename(video_file))[0])\n",
    "        os.makedirs(keyframe_path, exist_ok=True)\n",
    "        try:\n",
    "            detect_shot_boundaries(video_file, output_file, keyframe_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {video_file}: {e}\")\n",
    "\n",
    "def get_video_files(input_directory):\n",
    "    video_files = []\n",
    "    for root, _, files in os.walk(input_directory):\n",
    "        for file in files:\n",
    "            if file.endswith(('.mp4', '.avi', '.mov', '.mkv')):\n",
    "                video_files.append(os.path.join(root, file))\n",
    "    return video_files\n",
    "\n",
    "print(\"Starting shot boundary detection...\")\n",
    "video_files = get_video_files(input_dir)\n",
    "print(f\"Found {len(video_files)} video files to process.\")\n",
    "print(\"Video files:\", video_files)\n",
    "process_videos(video_files, output_dir, keyframe_dir)\n",
    "print(\"Shot boundary detection completed successfully.\")"
   ],
   "id": "81d3ae9c904bc3e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Feature Extraction",
   "id": "edb9ac3c1e364162"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Configuration\n",
    "keyframe_dir = 'keyframes/'\n",
    "db_name = 'video_features_db'\n",
    "collection_name = 'features'\n",
    "batch_size = 32\n",
    "yolo_model_path = 'yolov5s.pt'  # Using the smallest version of YOLOv5 for demonstration\n",
    "\n",
    "# Configuration Google Colab\n",
    "# keyframe_dir = '/content/drive/MyDrive/keyframes'\n",
    "# db_name = 'video_features_db'\n",
    "# collection_name = 'features'\n",
    "# batch_size = 32\n",
    "# yolo_model_path = '/content/drive/MyDrive/yolov5s.pt'  # Using the smallest version of YOLOv5 for demonstration\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Initialize MongoDB client\n",
    "client = MongoClient('localhost', 27017)\n",
    "db = client[db_name]\n",
    "collection = db[collection_name]\n",
    "\n",
    "# Initialize pre-trained models\n",
    "vgg_model = VGG16(weights='imagenet', include_top=False, pooling='avg')\n",
    "resnet_model = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n",
    "\n",
    "# Load YOLOv5 model\n",
    "yolo_model = torch.hub.load('ultralytics/yolov5', 'custom', path=yolo_model_path)\n",
    "\n",
    "def extract_features(model, preprocess_input, img):\n",
    "    try:\n",
    "        img = cv2.resize(img, (224, 224))\n",
    "        img = img.astype('float32')\n",
    "        img = preprocess_input(img)\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        features = model.predict(img)\n",
    "        return features.flatten()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting features: {e}\")\n",
    "        return None\n",
    "\n",
    "def detect_objects_yolo(img):\n",
    "    try:\n",
    "        results = yolo_model(img)\n",
    "        detected_objects = results.pandas().xyxy[0].to_dict(orient=\"records\")\n",
    "        return detected_objects\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error detecting objects with YOLO: {e}\")\n",
    "        return []\n",
    "\n",
    "def process_keyframes(keyframe_directory, model, preprocess_input, model_name):\n",
    "    try:\n",
    "        for root, _, files in os.walk(keyframe_directory):\n",
    "            for file in tqdm(files, desc=f\"Extracting features using {model_name}\", unit=\"frame\"):\n",
    "                if file.endswith('.jpg'):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    img = cv2.imread(file_path)\n",
    "                    \n",
    "                    # YOLO Object Detection\n",
    "                    objects = detect_objects_yolo(img)\n",
    "                    \n",
    "                    # CNN Feature Extraction\n",
    "                    features = extract_features(model, preprocess_input, img)\n",
    "                    \n",
    "                    if features is not None:\n",
    "                        video_id, frame_id = os.path.basename(root), os.path.splitext(file)[0]\n",
    "                        feature_data = {\n",
    "                            'video_id': video_id,\n",
    "                            'frame_id': frame_id,\n",
    "                            'model': model_name,\n",
    "                            'features': features.tolist(),\n",
    "                            'objects': objects\n",
    "                        }\n",
    "                        collection.insert_one(feature_data)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing keyframes: {e}\")\n",
    "\n",
    "def process_videos(keyframe_directory):\n",
    "    process_keyframes(keyframe_directory, vgg_model, preprocess_input_vgg, 'VGG16')\n",
    "    process_keyframes(keyframe_directory, resnet_model, preprocess_input_resnet, 'ResNet50')\n",
    "\n",
    "logging.info(\"Starting feature extraction with YOLOv5 integration...\")\n",
    "process_videos(keyframe_dir)\n",
    "logging.info(\"Feature extraction with YOLOv5 integration completed successfully.\")\n"
   ],
   "id": "49fe172e86dd68c9",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
