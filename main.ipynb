{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Imports",
   "id": "32d736dd469ee957"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T14:32:30.815157Z",
     "start_time": "2024-05-19T14:32:08.829981Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import logging\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from pymongo import MongoClient\n",
    "from scenedetect import VideoManager, SceneManager\n",
    "from scenedetect.detectors import ContentDetector, ThresholdDetector\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input as preprocess_input_resnet\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input as preprocess_input_vgg\n",
    "from tqdm import tqdm\n"
   ],
   "id": "3918caaa9649f9c2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-19 16:32:16.416177: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Video Pre-processing",
   "id": "e22e91d1016abf2e"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-19T17:26:58.172297Z",
     "start_time": "2024-05-19T15:07:20.018279Z"
    }
   },
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuration\n",
    "input_dir = 'V3C1-100/'\n",
    "output_dir = 'preprocessed_videos/'\n",
    "output_format = 'mp4'\n",
    "resize_width = 640\n",
    "resize_height = 480\n",
    "convert_to_grayscale = False\n",
    "frame_rate = 24  # Target frame rate\n",
    "max_workers = 1\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def preprocess_video(input_path, output_path, resize_dim, grayscale, frame_rate):\n",
    "    try:\n",
    "        print(f\"Processing video: {input_path}\")\n",
    "        cap = cv2.VideoCapture(input_path)\n",
    "        if not cap.isOpened():\n",
    "            raise ValueError(f\"Failed to open video file: {input_path}\")\n",
    "\n",
    "        original_frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "        if original_frame_rate == 0:\n",
    "            raise ValueError(f\"Failed to get frame rate for video file: {input_path}\")\n",
    "\n",
    "        frame_interval = int(original_frame_rate // frame_rate)\n",
    "\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(output_path, fourcc, frame_rate, resize_dim, not grayscale)\n",
    "\n",
    "        frame_count = 0\n",
    "        prev_gray = None\n",
    "\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            if frame_count % frame_interval == 0:\n",
    "                # Resize frame\n",
    "                frame = cv2.resize(frame, resize_dim, interpolation=cv2.INTER_AREA)\n",
    "                # Convert to grayscale if needed for optical flow\n",
    "                gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "                # Apply noise reduction\n",
    "                frame = cv2.fastNlMeansDenoisingColored(frame, None, 10, 10, 7, 21)\n",
    "                # Apply histogram equalization\n",
    "                if grayscale:\n",
    "                    frame = cv2.equalizeHist(frame)\n",
    "                else:\n",
    "                    for i in range(3):\n",
    "                        frame[:, :, i] = cv2.equalizeHist(frame[:, :, i])\n",
    "                # # Edge detection\n",
    "                # edges = cv2.Canny(frame, 100, 200)\n",
    "\n",
    "                # Optical flow calculation\n",
    "                if prev_gray is not None:\n",
    "                    flow = cv2.calcOpticalFlowFarneback(prev_gray, gray_frame, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "                    mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "                    hsv = np.zeros_like(frame)\n",
    "                    hsv[..., 1] = 255\n",
    "                    hsv[..., 0] = ang * 180 / np.pi / 2\n",
    "                    hsv[..., 2] = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX)\n",
    "                    optical_flow = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "                    frame = cv2.addWeighted(frame, 0.5, optical_flow, 0.5, 0)\n",
    "                prev_gray = gray_frame\n",
    "\n",
    "                # # Feature extraction using HOG\n",
    "                # hog = cv2.HOGDescriptor()\n",
    "                # hog_features = hog.compute(frame)\n",
    "\n",
    "                # Keypoint descriptors (ORB example)\n",
    "                orb = cv2.ORB_create()\n",
    "                kp, des = orb.detectAndCompute(frame, None)\n",
    "                frame = cv2.drawKeypoints(frame, kp, None, color=(0, 255, 0), flags=0)\n",
    "                \n",
    "                out.write(frame)\n",
    "            frame_count += 1\n",
    "\n",
    "        cap.release()\n",
    "        out.release()\n",
    "        print(f\"Successfully processed video: {input_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {input_path}: {e}\")\n",
    "\n",
    "def get_video_files(input_directory):\n",
    "    video_files = []\n",
    "    for root, _, files in os.walk(input_directory):\n",
    "        for file in files:\n",
    "            if file.endswith(('.mp4', '.avi', '.mov', '.mkv')):\n",
    "                video_files.append(os.path.join(root, file))\n",
    "    return video_files\n",
    "\n",
    "def process_videos(video_files, output_directory, resize_dim, grayscale, frame_rate):\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = []\n",
    "        with tqdm(total=len(video_files), desc=\"Processing Videos\", unit=\"video\") as pbar:\n",
    "            for video_file in video_files:\n",
    "                relative_path = os.path.relpath(video_file, input_dir)\n",
    "                output_file = os.path.join(output_directory, os.path.splitext(relative_path)[0] + '.' + output_format)\n",
    "                os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "                future = executor.submit(preprocess_video, video_file, output_file, resize_dim, grayscale, frame_rate)\n",
    "                futures.append(future)\n",
    "\n",
    "            for future in futures:\n",
    "                future.add_done_callback(lambda p: pbar.update())\n",
    "            for future in futures:\n",
    "                future.result()  # Wait for all threads to complete\n",
    "\n",
    "print(\"Starting video pre-processing...\")\n",
    "video_files = get_video_files(input_dir)\n",
    "print(f\"Found {len(video_files)} video files.\")\n",
    "resize_dim = (resize_width, resize_height)\n",
    "process_videos(video_files, output_dir, resize_dim, convert_to_grayscale, frame_rate)\n",
    "print(\"All videos processed successfully.\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting video pre-processing...\n",
      "Found 100 video files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Videos:   0%|          | 0/100 [00:00<?, ?video/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video: V3C1-100/00120/00120.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NULL @ 0x7fbc96a26900] missing picture in access unit with size 5\n",
      "Processing Videos:   0%|          | 0/100 [2:19:37<?, ?video/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 112\u001B[0m, in \u001B[0;36mprocess_videos\u001B[0;34m(video_files, output_directory, resize_dim, grayscale, frame_rate)\u001B[0m\n\u001B[1;32m    111\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m future \u001B[38;5;129;01min\u001B[39;00m futures:\n\u001B[0;32m--> 112\u001B[0m     \u001B[43mfuture\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresult\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py:451\u001B[0m, in \u001B[0;36mFuture.result\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    449\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__get_result()\n\u001B[0;32m--> 451\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_condition\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    453\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_state \u001B[38;5;129;01min\u001B[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/threading.py:355\u001B[0m, in \u001B[0;36mCondition.wait\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    354\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 355\u001B[0m     \u001B[43mwaiter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43macquire\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    356\u001B[0m     gotit \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 118\u001B[0m\n\u001B[1;32m    116\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFound \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(video_files)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m video files.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    117\u001B[0m resize_dim \u001B[38;5;241m=\u001B[39m (resize_width, resize_height)\n\u001B[0;32m--> 118\u001B[0m \u001B[43mprocess_videos\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvideo_files\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresize_dim\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert_to_grayscale\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe_rate\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    119\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAll videos processed successfully.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[10], line 99\u001B[0m, in \u001B[0;36mprocess_videos\u001B[0;34m(video_files, output_directory, resize_dim, grayscale, frame_rate)\u001B[0m\n\u001B[1;32m     98\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mprocess_videos\u001B[39m(video_files, output_directory, resize_dim, grayscale, frame_rate):\n\u001B[0;32m---> 99\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mwith\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mThreadPoolExecutor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmax_workers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_workers\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mas\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mexecutor\u001B[49m\u001B[43m:\u001B[49m\n\u001B[1;32m    100\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfutures\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[43m]\u001B[49m\n\u001B[1;32m    101\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mwith\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtqdm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtotal\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mvideo_files\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdesc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mProcessing Videos\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43munit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mvideo\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mas\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mpbar\u001B[49m\u001B[43m:\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py:647\u001B[0m, in \u001B[0;36mExecutor.__exit__\u001B[0;34m(self, exc_type, exc_val, exc_tb)\u001B[0m\n\u001B[1;32m    646\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__exit__\u001B[39m(\u001B[38;5;28mself\u001B[39m, exc_type, exc_val, exc_tb):\n\u001B[0;32m--> 647\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshutdown\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwait\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m    648\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/thread.py:238\u001B[0m, in \u001B[0;36mThreadPoolExecutor.shutdown\u001B[0;34m(self, wait, cancel_futures)\u001B[0m\n\u001B[1;32m    236\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m wait:\n\u001B[1;32m    237\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads:\n\u001B[0;32m--> 238\u001B[0m         \u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/threading.py:1147\u001B[0m, in \u001B[0;36mThread.join\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m   1144\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcannot join current thread\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1146\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1147\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_wait_for_tstate_lock\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1148\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1149\u001B[0m     \u001B[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001B[39;00m\n\u001B[1;32m   1150\u001B[0m     \u001B[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001B[39;00m\n\u001B[1;32m   1151\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_wait_for_tstate_lock(timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mmax\u001B[39m(timeout, \u001B[38;5;241m0\u001B[39m))\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/threading.py:1167\u001B[0m, in \u001B[0;36mThread._wait_for_tstate_lock\u001B[0;34m(self, block, timeout)\u001B[0m\n\u001B[1;32m   1164\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[1;32m   1166\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1167\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mlock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43macquire\u001B[49m\u001B[43m(\u001B[49m\u001B[43mblock\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m   1168\u001B[0m         lock\u001B[38;5;241m.\u001B[39mrelease()\n\u001B[1;32m   1169\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stop()\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Shot Boundary Detection",
   "id": "f756ce17872689ef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Configuration\n",
    "input_dir = 'preprocessed_videos/'\n",
    "output_dir = 'shot_boundaries/'\n",
    "keyframe_dir = 'keyframes/'\n",
    "min_scene_length = 15  # Minimum length of a scene in frames\n",
    "threshold = 30.0  # Threshold for the ThresholdDetector\n",
    "min_scene_len = 2  # Minimum number of frames a scene should last\n",
    "hist_threshold = 0.4  # Threshold for histogram comparison\n",
    "\n",
    "# Ensure the output and keyframe directories exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(keyframe_dir, exist_ok=True)\n",
    "\n",
    "def calculate_histogram_difference(frame1, frame2):\n",
    "    hist1 = cv2.calcHist([frame1], [0], None, [256], [0, 256])\n",
    "    hist2 = cv2.calcHist([frame2], [0], None, [256], [0, 256])\n",
    "    cv2.normalize(hist1, hist1)\n",
    "    cv2.normalize(hist2, hist2)\n",
    "    return cv2.compareHist(hist1, hist2, cv2.HISTCMP_CORREL)\n",
    "\n",
    "def detect_shot_boundaries(video_path, output_path, keyframe_path):\n",
    "    video_manager = VideoManager([video_path])\n",
    "    scene_manager = SceneManager()\n",
    "\n",
    "    # Add ContentDetector and ThresholdDetector\n",
    "    scene_manager.add_detector(ContentDetector(threshold=30.0, min_scene_len=min_scene_length))\n",
    "    scene_manager.add_detector(ThresholdDetector(threshold=threshold, min_scene_len=min_scene_len))\n",
    "\n",
    "    video_manager.set_downscale_factor()\n",
    "    video_manager.start()\n",
    "    scene_manager.detect_scenes(frame_source=video_manager)\n",
    "    scenes = scene_manager.get_scene_list()\n",
    "\n",
    "    # Additional processing for gradual transitions\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    prev_frame = None\n",
    "    prev_gray = None\n",
    "    frame_num = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        if prev_frame is not None:\n",
    "            hist_diff = calculate_histogram_difference(prev_frame, frame)\n",
    "            if hist_diff < hist_threshold:\n",
    "                # Gradual transition detected\n",
    "                scenes.append((frame_num, frame_num + min_scene_len))\n",
    "            # Motion analysis using optical flow\n",
    "            if prev_gray is not None:\n",
    "                flow = cv2.calcOpticalFlowFarneback(prev_gray, gray_frame, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "                mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "                motion_magnitude = np.mean(mag)\n",
    "                if motion_magnitude > threshold:\n",
    "                    scenes.append((frame_num, frame_num + min_scene_len))\n",
    "        prev_frame = frame\n",
    "        prev_gray = gray_frame\n",
    "        frame_num += 1\n",
    "    cap.release()\n",
    "\n",
    "    # Remove duplicate and sort scenes\n",
    "    scenes = sorted(list(set(scenes)))\n",
    "\n",
    "    # Save shot boundaries to a file\n",
    "    with open(output_path, 'w') as f:\n",
    "        for start, end in scenes:\n",
    "            f.write(f\"{start},{end}\\n\")\n",
    "\n",
    "    # Extract keyframes for each detected scene\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    for start, end in scenes:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, start)\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            keyframe_filename = os.path.join(keyframe_path, f\"{os.path.basename(video_path)}_frame_{start}.jpg\")\n",
    "            cv2.imwrite(keyframe_filename, frame)\n",
    "    cap.release()\n",
    "\n",
    "def process_videos(video_files, output_directory, keyframe_directory):\n",
    "    for video_file in tqdm(video_files, desc=\"Detecting Shot Boundaries\", unit=\"video\"):\n",
    "        output_file = os.path.join(output_directory, os.path.splitext(os.path.basename(video_file))[0] + '_shots.txt')\n",
    "        keyframe_path = os.path.join(keyframe_directory, os.path.splitext(os.path.basename(video_file))[0])\n",
    "        os.makedirs(keyframe_path, exist_ok=True)\n",
    "        detect_shot_boundaries(video_file, output_file, keyframe_path)\n",
    "\n",
    "def get_video_files(input_directory):\n",
    "    video_files = []\n",
    "    for root, _, files in os.walk(input_directory):\n",
    "        for file in files:\n",
    "            if file.endswith(('.mp4', '.avi', '.mov', '.mkv')):\n",
    "                video_files.append(os.path.join(root, file))\n",
    "    return video_files\n",
    "\n",
    "print(\"Starting shot boundary detection...\")\n",
    "video_files = get_video_files(input_dir)\n",
    "process_videos(video_files, output_dir, keyframe_dir)\n",
    "print(\"Shot boundary detection completed successfully.\")\n"
   ],
   "id": "81d3ae9c904bc3e2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Feature Extraction",
   "id": "edb9ac3c1e364162"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Configuration\n",
    "keyframe_dir = 'keyframes/'\n",
    "db_name = 'video_features_db'\n",
    "collection_name = 'features'\n",
    "batch_size = 32\n",
    "yolo_model_path = 'yolov5s.pt'  # Using the smallest version of YOLOv5 for demonstration\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Initialize MongoDB client\n",
    "client = MongoClient('localhost', 27017)\n",
    "db = client[db_name]\n",
    "collection = db[collection_name]\n",
    "\n",
    "# Initialize pre-trained models\n",
    "vgg_model = VGG16(weights='imagenet', include_top=False, pooling='avg')\n",
    "resnet_model = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n",
    "\n",
    "# Load YOLOv5 model\n",
    "yolo_model = torch.hub.load('ultralytics/yolov5', 'custom', path=yolo_model_path)\n",
    "\n",
    "def extract_features(model, preprocess_input, img):\n",
    "    try:\n",
    "        img = cv2.resize(img, (224, 224))\n",
    "        img = img.astype('float32')\n",
    "        img = preprocess_input(img)\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        features = model.predict(img)\n",
    "        return features.flatten()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting features: {e}\")\n",
    "        return None\n",
    "\n",
    "def detect_objects_yolo(img):\n",
    "    try:\n",
    "        results = yolo_model(img)\n",
    "        detected_objects = results.pandas().xyxy[0].to_dict(orient=\"records\")\n",
    "        return detected_objects\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error detecting objects with YOLO: {e}\")\n",
    "        return []\n",
    "\n",
    "def process_keyframes(keyframe_directory, model, preprocess_input, model_name):\n",
    "    try:\n",
    "        for root, _, files in os.walk(keyframe_directory):\n",
    "            for file in tqdm(files, desc=f\"Extracting features using {model_name}\", unit=\"frame\"):\n",
    "                if file.endswith('.jpg'):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    img = cv2.imread(file_path)\n",
    "                    \n",
    "                    # YOLO Object Detection\n",
    "                    objects = detect_objects_yolo(img)\n",
    "                    \n",
    "                    # CNN Feature Extraction\n",
    "                    features = extract_features(model, preprocess_input, img)\n",
    "                    \n",
    "                    if features is not None:\n",
    "                        video_id, frame_id = os.path.basename(root), os.path.splitext(file)[0]\n",
    "                        feature_data = {\n",
    "                            'video_id': video_id,\n",
    "                            'frame_id': frame_id,\n",
    "                            'model': model_name,\n",
    "                            'features': features.tolist(),\n",
    "                            'objects': objects\n",
    "                        }\n",
    "                        collection.insert_one(feature_data)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing keyframes: {e}\")\n",
    "\n",
    "def process_videos(keyframe_directory):\n",
    "    process_keyframes(keyframe_directory, vgg_model, preprocess_input_vgg, 'VGG16')\n",
    "    process_keyframes(keyframe_directory, resnet_model, preprocess_input_resnet, 'ResNet50')\n",
    "\n",
    "logging.info(\"Starting feature extraction with YOLOv5 integration...\")\n",
    "process_videos(keyframe_dir)\n",
    "logging.info(\"Feature extraction with YOLOv5 integration completed successfully.\")\n"
   ],
   "id": "49fe172e86dd68c9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
