{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Imports",
   "id": "32d736dd469ee957"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Google Colab only\n",
    "'''\n",
    "!pip install pymongo scenedetect\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "'''"
   ],
   "id": "2d000ec8ed22235f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Activate GPU\n",
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ],
   "id": "20dad6f04b40918d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# More Memory\n",
    "from psutil import virtual_memory\n",
    "ram_gb = virtual_memory().total / 1e9\n",
    "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
    "\n",
    "if ram_gb < 20:\n",
    "  print('Not using a high-RAM runtime')\n",
    "else:\n",
    "  print('You are using a high-RAM runtime!')"
   ],
   "id": "2a47ec729360bd62",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import logging\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import cv2\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "from pymongo import MongoClient\n",
    "from scenedetect import VideoManager, SceneManager\n",
    "from scenedetect.detectors import ContentDetector, ThresholdDetector\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input as preprocess_input_resnet\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input as preprocess_input_vgg\n",
    "from tqdm import tqdm\n",
    "\n"
   ],
   "id": "3918caaa9649f9c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Video Pre-processing",
   "id": "e22e91d1016abf2e"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Configuration\n",
    "input_dir = 'V3C1-100/'\n",
    "output_dir = 'preprocessed_videos/'\n",
    "output_format = 'mp4'\n",
    "\n",
    "# Configuration Google Colab\n",
    "# input_dir = '/content/drive/MyDrive/V3C1-100'\n",
    "# output_dir = '/content/drive/MyDrive/preprocessed_videos'\n",
    "# output_format = 'mp4'\n",
    "\n",
    "resize_width = 640\n",
    "resize_height = 480\n",
    "convert_to_grayscale = False\n",
    "frame_rate = 24  # Target frame rate\n",
    "max_workers = 1\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def preprocess_video(input_path, output_path, resize_dim, grayscale, frame_rate):\n",
    "    try:\n",
    "        print(f\"Processing video: {input_path}\")\n",
    "        cap = cv2.VideoCapture(input_path)\n",
    "        if not cap.isOpened():\n",
    "            raise ValueError(f\"Failed to open video file: {input_path}\")\n",
    "\n",
    "        original_frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "        if original_frame_rate == 0:\n",
    "            raise ValueError(f\"Failed to get frame rate for video file: {input_path}\")\n",
    "\n",
    "        frame_interval = int(original_frame_rate // frame_rate)\n",
    "\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(output_path, fourcc, frame_rate, resize_dim, not grayscale)\n",
    "\n",
    "        frame_count = 0\n",
    "        prev_gray = None\n",
    "\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            if frame_count % frame_interval == 0:\n",
    "                # Resize frame\n",
    "                frame = cv2.resize(frame, resize_dim, interpolation=cv2.INTER_AREA)\n",
    "                # # Convert to grayscale if needed for optical flow\n",
    "                # gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "                # # Apply noise reduction (computationally expensive)\n",
    "                # frame = cv2.fastNlMeansDenoisingColored(frame, None, 10, 10, 7, 21)\n",
    "                # # Apply histogram equalization\n",
    "                # if grayscale:\n",
    "                #     frame = cv2.equalizeHist(frame)\n",
    "                # else:\n",
    "                #     for i in range(3):\n",
    "                #         frame[:, :, i] = cv2.equalizeHist(frame[:, :, i])\n",
    "                # # Edge detection\n",
    "                # edges = cv2.Canny(frame, 100, 200)\n",
    "\n",
    "                # # Optical flow calculation\n",
    "                # if prev_gray is not None:\n",
    "                #     flow = cv2.calcOpticalFlowFarneback(prev_gray, gray_frame, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "                #     mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "                #     hsv = np.zeros_like(frame)\n",
    "                #     hsv[..., 1] = 255\n",
    "                #     hsv[..., 0] = ang * 180 / np.pi / 2\n",
    "                #     hsv[..., 2] = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX)\n",
    "                #     optical_flow = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "                #     frame = cv2.addWeighted(frame, 0.5, optical_flow, 0.5, 0)\n",
    "                # prev_gray = gray_frame\n",
    "\n",
    "                # # Feature extraction using HOG\n",
    "                # hog = cv2.HOGDescriptor()\n",
    "                # hog_features = hog.compute(frame)\n",
    "\n",
    "                # # Keypoint descriptors (ORB example)\n",
    "                # orb = cv2.ORB_create()\n",
    "                # kp, des = orb.detectAndCompute(frame, None)\n",
    "                # frame = cv2.drawKeypoints(frame, kp, None, color=(0, 255, 0), flags=0)\n",
    "                \n",
    "                out.write(frame)\n",
    "            frame_count += 1\n",
    "\n",
    "        cap.release()\n",
    "        out.release()\n",
    "        print(f\"Successfully processed video: {input_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {input_path}: {e}\")\n",
    "\n",
    "def get_video_files(input_directory):\n",
    "    video_files = []\n",
    "    for root, _, files in os.walk(input_directory):\n",
    "        for file in files:\n",
    "            if file.endswith(('.mp4', '.avi', '.mov', '.mkv')):\n",
    "                video_files.append(os.path.join(root, file))\n",
    "    return video_files\n",
    "\n",
    "def process_videos(video_files, output_directory, resize_dim, grayscale, frame_rate):\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = []\n",
    "        with tqdm(total=len(video_files), desc=\"Processing Videos\", unit=\"video\") as pbar:\n",
    "            for video_file in video_files:\n",
    "                relative_path = os.path.relpath(video_file, input_dir)\n",
    "                output_file = os.path.join(output_directory, os.path.splitext(relative_path)[0] + '.' + output_format)\n",
    "                os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "                future = executor.submit(preprocess_video, video_file, output_file, resize_dim, grayscale, frame_rate)\n",
    "                futures.append(future)\n",
    "\n",
    "            for future in futures:\n",
    "                future.add_done_callback(lambda p: pbar.update())\n",
    "            for future in futures:\n",
    "                future.result()  # Wait for all threads to complete\n",
    "\n",
    "print(\"Starting video pre-processing...\")\n",
    "video_files = get_video_files(input_dir)\n",
    "print(f\"Found {len(video_files)} video files.\")\n",
    "resize_dim = (resize_width, resize_height)\n",
    "process_videos(video_files, output_dir, resize_dim, convert_to_grayscale, frame_rate)\n",
    "print(\"All videos processed successfully.\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Shot Boundary Detection",
   "id": "f756ce17872689ef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Configuration\n",
    "input_dir = 'preprocessed_videos/'\n",
    "output_dir = 'shot_boundaries/'\n",
    "keyframe_dir = 'keyframes/'\n",
    "\n",
    "# Configuration Google Colab\n",
    "# input_dir = '/content/drive/MyDrive/preprocessed_videos'\n",
    "# output_dir = '/content/drive/MyDrive/shot_boundaries'\n",
    "# keyframe_dir = '/content/drive/MyDrive/keyframes'\n",
    "\n",
    "min_scene_length = 15  # Minimum length of a scene in frames\n",
    "threshold = 30.0  # Threshold for the ThresholdDetector\n",
    "min_scene_len = 2  # Minimum number of frames a scene should last\n",
    "hist_threshold = 0.4  # Threshold for histogram comparison\n",
    "\n",
    "# Ensure the output and keyframe directories exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(keyframe_dir, exist_ok=True)\n",
    "\n",
    "def calculate_histogram_difference(frame1, frame2):\n",
    "    hist1 = cv2.calcHist([frame1], [0], None, [256], [0, 256])\n",
    "    hist2 = cv2.calcHist([frame2], [0], None, [256], [0, 256])\n",
    "    cv2.normalize(hist1, hist1)\n",
    "    cv2.normalize(hist2, hist2)\n",
    "    return cv2.compareHist(hist1, hist2, cv2.HISTCMP_CORREL)\n",
    "\n",
    "def detect_shot_boundaries(video_path, output_path, keyframe_path):\n",
    "    video_manager = VideoManager([video_path])\n",
    "    scene_manager = SceneManager()\n",
    "\n",
    "    # Add ContentDetector and ThresholdDetector\n",
    "    scene_manager.add_detector(ContentDetector(threshold=30.0, min_scene_len=min_scene_length))\n",
    "    scene_manager.add_detector(ThresholdDetector(threshold=threshold, min_scene_len=min_scene_len))\n",
    "\n",
    "    video_manager.set_downscale_factor()\n",
    "    video_manager.start()\n",
    "    scene_manager.detect_scenes(frame_source=video_manager)\n",
    "    scenes = scene_manager.get_scene_list()\n",
    "    print(f\"Detected {len(scenes)} scenes in video {video_path}\")\n",
    "\n",
    "    # Additional processing for gradual transitions\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    prev_frame = None\n",
    "    prev_gray = None\n",
    "    frame_num = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        if prev_frame is not None:\n",
    "            hist_diff = calculate_histogram_difference(prev_frame, frame)\n",
    "            if hist_diff < hist_threshold:\n",
    "                # Gradual transition detected\n",
    "                scenes.append((frame_num, frame_num + min_scene_len))\n",
    "            # Motion analysis using optical flow\n",
    "            if prev_gray is not None:\n",
    "                flow = cv2.calcOpticalFlowFarneback(prev_gray, gray_frame, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "                mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "                motion_magnitude = np.mean(mag)\n",
    "                if motion_magnitude > threshold:\n",
    "                    scenes.append((frame_num, frame_num + min_scene_len))\n",
    "        prev_frame = frame\n",
    "        prev_gray = gray_frame\n",
    "        frame_num += 1\n",
    "    cap.release()\n",
    "\n",
    "    # Remove duplicate and sort scenes\n",
    "    scenes = sorted(list(set(scenes)))\n",
    "    print(f\"Total scenes after processing: {len(scenes)}\")\n",
    "\n",
    "    # Save shot boundaries to a file\n",
    "    with open(output_path, 'w') as f:\n",
    "        for start_time, end_time in scenes:\n",
    "            f.write(f\"{start_time}, {end_time}\\n\")\n",
    "            # f.write(f\"{start_time.get_seconds()}, {end_time.get_seconds()}\\n\")\n",
    "            # f.write(f\"{start_time.get_frames()}, {end_time.get_frames()}\\n\")\n",
    "        print(f\"Shot boundaries saved to {output_path}\")\n",
    "\n",
    "    # Extract keyframes for each detected scene\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    for start, end in scenes:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, int(start))\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            keyframe_filename = os.path.join(keyframe_path, f\"{os.path.basename(video_path)}_start_{start}.jpg\")\n",
    "            cv2.imwrite(keyframe_filename, frame)\n",
    "    cap.release()\n",
    "    print(f\"Keyframes saved to {keyframe_path}\")\n",
    "\n",
    "\n",
    "def process_videos(video_files, output_directory, keyframe_directory):\n",
    "    for video_file in tqdm(video_files, desc=\"Detecting Shot Boundaries\", unit=\"video\"):\n",
    "        output_file = os.path.join(output_directory, os.path.splitext(os.path.basename(video_file))[0] + '_shots.txt')\n",
    "        keyframe_path = os.path.join(keyframe_directory, os.path.splitext(os.path.basename(video_file))[0])\n",
    "        os.makedirs(keyframe_path, exist_ok=True)\n",
    "        try:\n",
    "            detect_shot_boundaries(video_file, output_file, keyframe_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {video_file}: {e}\")\n",
    "\n",
    "def get_video_files(input_directory):\n",
    "    video_files = []\n",
    "    for root, _, files in os.walk(input_directory):\n",
    "        for file in files:\n",
    "            if file.endswith(('.mp4', '.avi', '.mov', '.mkv')):\n",
    "                match = re.search(r'\\d+', file)\n",
    "                if match:\n",
    "                    number = int(match.group())\n",
    "                    if 126 <= number <= 149:\n",
    "                        video_files.append(os.path.join(root, file))\n",
    "    return video_files\n",
    "\n",
    "\n",
    "print(\"Starting shot boundary detection...\")\n",
    "video_files = get_video_files(input_dir)\n",
    "print(f\"Found {len(video_files)} video files to process.\")\n",
    "print(\"Video files:\", video_files)\n",
    "process_videos(video_files, output_dir, keyframe_dir)\n",
    "print(\"Shot boundary detection completed successfully.\") "
   ],
   "id": "81d3ae9c904bc3e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Feature Extraction",
   "id": "edb9ac3c1e364162"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Using YOLOv5",
   "id": "2b75a47c4921da9d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from sklearn.metrics import confusion_matrix, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Ensure the correct paths to your YOLO files\n",
    "yolo_cfg_path = \"weights/yolov3.cfg\"  # Update this path if necessary\n",
    "yolo_weights_path = \"weights/yolov3.weights\"  # Update this path if necessary\n",
    "\n",
    "# Load YOLO\n",
    "net = cv2.dnn.readNet(yolo_weights_path, yolo_cfg_path)\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "# The full COCO class names for YOLOv3\n",
    "coco_classes = [\"person\", \"bicycle\", \"car\", \"motorbike\", \"aeroplane\", \"bus\", \"train\", \"truck\", \n",
    "                \"boat\", \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \n",
    "                \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \n",
    "                \"giraffe\", \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \n",
    "                \"skis\", \"snowboard\", \"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\", \n",
    "                \"skateboard\", \"surfboard\", \"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \n",
    "                \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\", \"sandwich\", \"orange\", \n",
    "                \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\", \"sofa\", \n",
    "                \"pottedplant\", \"bed\", \"diningtable\", \"toilet\", \"tvmonitor\", \"laptop\", \"mouse\", \n",
    "                \"remote\", \"keyboard\", \"cell phone\", \"microwave\", \"oven\", \"toaster\", \"sink\", \n",
    "                \"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\", \"hair drier\", \n",
    "                \"toothbrush\"]\n",
    "\n",
    "# The relevant classes for evaluation\n",
    "relevant_classes = [\"person\", \"bird\", \"truck\", \"horse\", \"car\"]\n",
    "\n",
    "# Mapping to unify bus and train as truck\n",
    "class_mapping = {\n",
    "    \"bus\": \"truck\",\n",
    "    \"train\": \"truck\"\n",
    "}\n",
    "\n",
    "# Path to annotated frames and annotations\n",
    "frame_dir = '00110'\n",
    "annotation_dir = '00110'\n",
    "\n",
    "def get_ground_truth_labels(xml_file):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    labels = []\n",
    "    boxes = []\n",
    "    for member in root.findall('object'):\n",
    "        label = member.find('name').text\n",
    "        if label in class_mapping:\n",
    "            label = class_mapping[label]\n",
    "        if label in relevant_classes:\n",
    "            labels.append(label)\n",
    "            bndbox = member.find('bndbox')\n",
    "            xmin = int(bndbox.find('xmin').text)\n",
    "            ymin = int(bndbox.find('ymin').text)\n",
    "            xmax = int(bndbox.find('xmax').text)\n",
    "            ymax = int(bndbox.find('ymax').text)\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "    return labels, boxes\n",
    "\n",
    "def iou(box1, box2):\n",
    "    \"\"\"Calculate Intersection Over Union (IOU) of two bounding boxes.\"\"\"\n",
    "    x1, y1, x2, y2 = box1\n",
    "    x1_p, y1_p, x2_p, y2_p = box2\n",
    "\n",
    "    xi1 = max(x1, x1_p)\n",
    "    yi1 = max(y1, y1_p)\n",
    "    xi2 = min(x2, x2_p)\n",
    "    yi2 = min(y2, y2_p)\n",
    "    inter_area = max(0, xi2 - xi1) * max(0, yi2 - yi1)\n",
    "\n",
    "    box1_area = (x2 - x1) * (y2 - y1)\n",
    "    box2_area = (x2_p - x1_p) * (y2_p - y1_p)\n",
    "    union_area = box1_area + box2_area - inter_area\n",
    "\n",
    "    iou = inter_area / union_area\n",
    "    return iou\n",
    "\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "for frame_name in os.listdir(frame_dir):\n",
    "    if frame_name.endswith(\".jpg\"):\n",
    "        frame_path = os.path.join(frame_dir, frame_name)\n",
    "        annotation_path = os.path.join(annotation_dir, frame_name.replace(\".jpg\", \".xml\"))\n",
    "        \n",
    "        img = cv2.imread(frame_path)\n",
    "        if img is None:\n",
    "            print(f\"Error reading image: {frame_path}\")\n",
    "            continue\n",
    "        height, width, channels = img.shape\n",
    "\n",
    "        # Get ground truth labels and boxes\n",
    "        if not os.path.exists(annotation_path):\n",
    "            print(f\"Annotation file not found: {annotation_path}\")\n",
    "            continue\n",
    "        gt_labels, gt_boxes = get_ground_truth_labels(annotation_path)\n",
    "        \n",
    "        # Detecting objects\n",
    "        blob = cv2.dnn.blobFromImage(img, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "        net.setInput(blob)\n",
    "        outs = net.forward(output_layers)\n",
    "\n",
    "        class_ids = []\n",
    "        confidences = []\n",
    "        boxes = []\n",
    "        for out in outs:\n",
    "            for detection in out:\n",
    "                scores = detection[5:]\n",
    "                class_id = np.argmax(scores)\n",
    "                confidence = scores[class_id]\n",
    "                if confidence > 0.5:\n",
    "                    center_x = int(detection[0] * width)\n",
    "                    center_y = int(detection[1] * height)\n",
    "                    w = int(detection[2] * width)\n",
    "                    h = int(detection[3] * height)\n",
    "\n",
    "                    x = int(center_x - w / 2)\n",
    "                    y = int(center_y - h / 2)\n",
    "\n",
    "                    boxes.append([x, y, w, h])\n",
    "                    confidences.append(float(confidence))\n",
    "                    class_ids.append(class_id)\n",
    "\n",
    "        indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "\n",
    "        frame_predicted_labels = []\n",
    "        detected_boxes = []\n",
    "        for i in range(len(boxes)):\n",
    "            if i in indexes:\n",
    "                x, y, w, h = boxes[i]\n",
    "                detected_box = [x, y, x + w, y + h]\n",
    "                \n",
    "                # Map detected class_id to relevant classes, considering the mapping\n",
    "                detected_class = coco_classes[class_ids[i]]\n",
    "                if detected_class in class_mapping:\n",
    "                    detected_class = class_mapping[detected_class]\n",
    "                if detected_class in relevant_classes:\n",
    "                    frame_predicted_labels.append(detected_class)\n",
    "                    detected_boxes.append(detected_box)\n",
    "                    \n",
    "                    # Drawing boxes on the image for visualization\n",
    "                    color = (0, 255, 0)\n",
    "                    cv2.rectangle(img, (x, y), (x + w, y + h), color, 2)\n",
    "                    cv2.putText(img, detected_class, (x, y + 30), cv2.FONT_HERSHEY_PLAIN, 3, color, 3)\n",
    "\n",
    "        # Collecting true and predicted labels\n",
    "        matched_predictions = [False] * len(detected_boxes)\n",
    "        for label, gt_box in zip(gt_labels, gt_boxes):\n",
    "            true_labels.append(label)\n",
    "            matched = False\n",
    "            for i, detected_box in enumerate(detected_boxes):\n",
    "                if iou(gt_box, detected_box) > 0.5:\n",
    "                    if not matched_predictions[i]:\n",
    "                        predicted_labels.append(frame_predicted_labels[i])\n",
    "                        matched_predictions[i] = True\n",
    "                        matched = True\n",
    "                        break\n",
    "            if not matched:\n",
    "                predicted_labels.append(\"none\")\n",
    "\n",
    "        for i, detected_box in enumerate(detected_boxes):\n",
    "            if not matched_predictions[i]:\n",
    "                true_labels.append(\"none\")\n",
    "                predicted_labels.append(frame_predicted_labels[i])\n",
    "\n",
    "        # Show the image with detections\n",
    "        cv2.imshow(\"Image\", img)\n",
    "        cv2.waitKey(1)  # Display each frame for a short time\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Print the true and predicted labels for debugging\n",
    "print(\"True Labels:\", true_labels)\n",
    "print(\"Predicted Labels:\", predicted_labels)\n",
    "\n",
    "# Evaluate Performance\n",
    "# Ensure we only have relevant class labels in the final lists\n",
    "filtered_true_labels = [label for label in true_labels if label in relevant_classes + [\"none\"]]\n",
    "filtered_predicted_labels = [label for label in predicted_labels if label in relevant_classes + [\"none\"]]\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(filtered_true_labels, filtered_predicted_labels, labels=relevant_classes + [\"none\"])\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Calculate recall for each class\n",
    "recall = recall_score(filtered_true_labels, filtered_predicted_labels, average=None, labels=relevant_classes)\n",
    "print(\"Recall for each class:\")\n",
    "print(recall)\n",
    "\n",
    "# Weighted average recall\n",
    "weighted_recall = recall_score(filtered_true_labels, filtered_predicted_labels, average='weighted', labels=relevant_classes)\n",
    "print(\"Weighted Recall:\")\n",
    "print(weighted_recall)\n",
    "\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", xticklabels=relevant_classes + [\"none\"], yticklabels=relevant_classes + [\"none\"], cmap=\"Blues\")\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ],
   "id": "610a0aee7df09116",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Using CLIP",
   "id": "ccdc94df61e64de"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from sklearn.metrics import confusion_matrix, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Load models and processors\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "faster_rcnn = fasterrcnn_resnet50_fpn(pretrained=True).to(device)\n",
    "faster_rcnn.eval()\n",
    "\n",
    "# Define the folder containing images\n",
    "frame_dir = '00110'\n",
    "annotation_dir = '00110'\n",
    "\n",
    "# Define the queries for evaluation\n",
    "queries = [\"A photo of a person\", \"A photo of a bird\", \"A photo of a truck\", \"A photo of a horse\", \"A photo of a car\"]\n",
    "query_labels = [\"person\", \"bird\", \"truck\", \"horse\", \"car\"]\n",
    "\n",
    "# Mapping to unify bus and train as truck\n",
    "class_mapping = {\n",
    "    \"bus\": \"truck\",\n",
    "    \"train\": \"truck\"\n",
    "}\n",
    "\n",
    "def get_ground_truth_labels(xml_file):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    labels = []\n",
    "    boxes = []\n",
    "    for member in root.findall('object'):\n",
    "        label = member.find('name').text\n",
    "        if label in class_mapping:\n",
    "            label = class_mapping[label]\n",
    "        if label in query_labels:\n",
    "            labels.append(label)\n",
    "            bndbox = member.find('bndbox')\n",
    "            xmin = int(bndbox.find('xmin').text)\n",
    "            ymin = int(bndbox.find('ymin').text)\n",
    "            xmax = int(bndbox.find('xmax').text)\n",
    "            ymax = int(bndbox.find('ymax').text)\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "    return labels, boxes\n",
    "\n",
    "def iou(box1, box2):\n",
    "    \"\"\"Calculate Intersection Over Union (IOU) of two bounding boxes.\"\"\"\n",
    "    x1, y1, x2, y2 = box1\n",
    "    x1_p, y1_p, x2_p, y2_p = box2\n",
    "\n",
    "    xi1 = max(x1, x1_p)\n",
    "    yi1 = max(y1, y1_p)\n",
    "    xi2 = min(x2, x2_p)\n",
    "    yi2 = min(y2, y2_p)\n",
    "    inter_area = max(0, xi2 - xi1) * max(0, yi2 - yi1)\n",
    "\n",
    "    box1_area = (x2 - x1) * (y2 - y1)\n",
    "    box2_area = (x2_p - x1_p) * (y2_p - y1_p)\n",
    "    union_area = box1_area + box2_area - inter_area\n",
    "\n",
    "    iou = inter_area / union_area\n",
    "    return iou\n",
    "\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "for frame_name in os.listdir(frame_dir):\n",
    "    if frame_name.endswith(\".jpg\"):\n",
    "        frame_path = os.path.join(frame_dir, frame_name)\n",
    "        annotation_path = os.path.join(annotation_dir, frame_name.replace(\".jpg\", \".xml\"))\n",
    "        \n",
    "        img = Image.open(frame_path).convert(\"RGB\")\n",
    "        if img is None:\n",
    "            print(f\"Error reading image: {frame_path}\")\n",
    "            continue\n",
    "        \n",
    "        # Get ground truth labels and boxes\n",
    "        if not os.path.exists(annotation_path):\n",
    "            print(f\"Annotation file not found: {annotation_path}\")\n",
    "            continue\n",
    "        gt_labels, gt_boxes = get_ground_truth_labels(annotation_path)\n",
    "\n",
    "        # Convert image to tensor\n",
    "        transform = T.Compose([T.ToTensor()])\n",
    "        img_tensor = transform(img).to(device)\n",
    "\n",
    "        # Generate bounding boxes using Faster R-CNN\n",
    "        with torch.no_grad():\n",
    "            predictions = faster_rcnn([img_tensor])\n",
    "        pred_boxes = predictions[0]['boxes'].cpu().numpy()\n",
    "        pred_scores = predictions[0]['scores'].cpu().numpy()\n",
    "\n",
    "        detected_labels = []\n",
    "        detected_boxes = []\n",
    "\n",
    "        for box, score in zip(pred_boxes, pred_scores):\n",
    "            if score > 0.5:  # Adjust threshold as needed\n",
    "                xmin, ymin, xmax, ymax = box\n",
    "                cropped_img = img.crop((xmin, ymin, xmax, ymax))\n",
    "                inputs = clip_processor(text=queries, images=cropped_img, return_tensors=\"pt\", padding=True).to(device)\n",
    "                outputs = clip_model(**inputs)\n",
    "                logits_per_image = outputs.logits_per_image.softmax(dim=1).detach().cpu().numpy().flatten()\n",
    "\n",
    "                best_idx = logits_per_image.argmax()\n",
    "                detected_label = query_labels[best_idx]\n",
    "                confidence = logits_per_image[best_idx]\n",
    "\n",
    "                detected_labels.append((detected_label, confidence, [xmin, ymin, xmax, ymax]))\n",
    "\n",
    "        # For visualization and evaluation\n",
    "        frame_true_labels = []\n",
    "        frame_predicted_labels = []\n",
    "        matched_predictions = [False] * len(detected_labels)\n",
    "\n",
    "        for label, gt_box in zip(gt_labels, gt_boxes):\n",
    "            frame_true_labels.append(label)\n",
    "            matched = False\n",
    "            for i, (detected_label, _, detected_box) in enumerate(detected_labels):\n",
    "                if iou(gt_box, detected_box) > 0.5 and not matched_predictions[i]:\n",
    "                    frame_predicted_labels.append(detected_label)\n",
    "                    matched_predictions[i] = True\n",
    "                    matched = True\n",
    "                    break\n",
    "            if not matched:\n",
    "                frame_predicted_labels.append(\"none\")\n",
    "\n",
    "        for i, (detected_label, _, detected_box) in enumerate(detected_labels):\n",
    "            if not matched_predictions[i]:\n",
    "                frame_true_labels.append(\"none\")\n",
    "                frame_predicted_labels.append(detected_label)\n",
    "\n",
    "        true_labels.extend(frame_true_labels)\n",
    "        predicted_labels.extend(frame_predicted_labels)\n",
    "\n",
    "# Print the true and predicted labels for debugging\n",
    "print(\"True Labels:\", true_labels)\n",
    "print(\"Predicted Labels:\", predicted_labels)\n",
    "\n",
    "# Ensure the lengths are equal\n",
    "min_length = min(len(true_labels), len(predicted_labels))\n",
    "filtered_true_labels = true_labels[:min_length]\n",
    "filtered_predicted_labels = predicted_labels[:min_length]\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(filtered_true_labels, filtered_predicted_labels, labels=query_labels + [\"none\"])\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Calculate recall for each class\n",
    "recall = recall_score(filtered_true_labels, filtered_predicted_labels, average=None, labels=query_labels)\n",
    "print(\"Recall for each class:\")\n",
    "print(recall)\n",
    "\n",
    "# Weighted average recall\n",
    "weighted_recall = recall_score(filtered_true_labels, filtered_predicted_labels, average='weighted', labels=query_labels)\n",
    "print(\"Weighted Recall:\")\n",
    "print(weighted_recall)\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", xticklabels=query_labels + [\"none\"], yticklabels=query_labels + [\"none\"], cmap=\"Blues\")\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ],
   "id": "e1b1a54782fe76e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Using CLIP and BLIP",
   "id": "60757cd80905baf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel, BlipProcessor, BlipForConditionalGeneration\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models.detection import FasterRCNN, fasterrcnn_resnet50_fpn\n",
    "import matplotlib.pyplot as plt\n",
    "from pymongo import MongoClient, errors\n",
    "import datetime\n",
    "\n",
    "# Define the folder containing images\n",
    "folder_path = \"keyframes/00102\"\n",
    "\n",
    "# Define the paths to the weights\n",
    "fasterrcnn_weights_path = \"weights/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\"\n",
    "resnet50_weights_path = \"weights/resnet50-0676ba61.pth\"\n",
    "\n",
    "# Check if the files exist\n",
    "assert os.path.exists(fasterrcnn_weights_path), \"Faster R-CNN weights file not found!\"\n",
    "assert os.path.exists(resnet50_weights_path), \"ResNet50 weights file not found!\"\n",
    "\n",
    "# Print paths to verify\n",
    "print(f\"Faster R-CNN weights path: {fasterrcnn_weights_path}\")\n",
    "print(f\"ResNet50 weights path: {resnet50_weights_path}\")\n",
    "\n",
    "# Load the ResNet50 backbone with local weights\n",
    "from torchvision.models import resnet50\n",
    "backbone = resnet50(pretrained=False)\n",
    "backbone_state_dict = torch.load(resnet50_weights_path, map_location=torch.device('cpu'))\n",
    "\n",
    "# Remove the fully connected layer weights from the state dictionary\n",
    "backbone_state_dict.pop(\"fc.weight\", None)\n",
    "backbone_state_dict.pop(\"fc.bias\", None)\n",
    "\n",
    "# Load the state dictionary with strict=False to ignore missing keys\n",
    "backbone.load_state_dict(backbone_state_dict, strict=False)\n",
    "\n",
    "# Create a custom backbone with FPN from the loaded ResNet50 backbone\n",
    "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
    "\n",
    "# Use the backbone with FPN, ensuring it uses the locally loaded weights\n",
    "backbone_with_fpn = resnet_fpn_backbone('resnet50', pretrained=False, norm_layer=torch.nn.BatchNorm2d)\n",
    "backbone_with_fpn.body.load_state_dict(backbone.state_dict(), strict=False)\n",
    "\n",
    "# Load the Faster R-CNN model with the custom backbone\n",
    "detection_model = FasterRCNN(backbone=backbone_with_fpn, num_classes=91)  # Use the backbone explicitly\n",
    "detection_model.load_state_dict(torch.load(fasterrcnn_weights_path, map_location=torch.device('cpu')))\n",
    "detection_model.eval()\n",
    "\n",
    "# Load the CLIP model and processor\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Load the BLIP captioning model and processor\n",
    "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "# MongoDB setup with error handling\n",
    "try:\n",
    "    client = MongoClient('mongodb://localhost:27017/', serverSelectionTimeoutMS=5000)\n",
    "    client.server_info()  # Trigger exception if cannot connect to db\n",
    "    db = client['object_detection']\n",
    "    collection = db['detected_objects']\n",
    "except errors.ServerSelectionTimeoutError as err:\n",
    "    print(\"Failed to connect to MongoDB server:\", err)\n",
    "    exit(1)\n",
    "\n",
    "# Transform for the object detection model\n",
    "transform = T.Compose([T.ToTensor()])\n",
    "\n",
    "# Function to generate captions using BLIP\n",
    "def generate_caption(image):\n",
    "    inputs = blip_processor(images=image, return_tensors=\"pt\")\n",
    "    out = blip_model.generate(**inputs)\n",
    "    caption = blip_processor.decode(out[0], skip_special_tokens=True)\n",
    "    return caption\n",
    "\n",
    "# Process each image in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "        image_path = os.path.join(folder_path, filename)\n",
    "        image = Image.open(image_path)\n",
    "\n",
    "        # Transform image for the detection model\n",
    "        image_tensor = transform(image)\n",
    "\n",
    "        # Get bounding boxes\n",
    "        with torch.no_grad():\n",
    "            detections = detection_model([image_tensor])[0]\n",
    "\n",
    "        # Filter out low-confidence detections\n",
    "        threshold = 0.5\n",
    "        boxes = [box for box, score in zip(detections['boxes'], detections['scores']) if score > threshold]\n",
    "\n",
    "        detected_objects = []\n",
    "\n",
    "        # Use BLIP to generate captions for objects within bounding boxes\n",
    "        for box in boxes:\n",
    "            xmin, ymin, xmax, ymax = box.int().numpy()\n",
    "            cropped_image = image.crop((xmin, ymin, xmax, ymax))\n",
    "            caption = generate_caption(cropped_image)\n",
    "            inputs = clip_processor(text=[caption], images=cropped_image, return_tensors=\"pt\", padding=True)\n",
    "            outputs = clip_model(**inputs)\n",
    "            probs = outputs.logits_per_image.softmax(dim=1).detach().cpu().numpy()[0]\n",
    "            detected_label = caption  # Use the generated caption as the label\n",
    "            confidence = probs.max()\n",
    "\n",
    "            detected_objects.append({\n",
    "                \"box\": [xmin, ymin, xmax, ymax],\n",
    "                \"label\": detected_label,\n",
    "                \"confidence\": float(confidence)\n",
    "            })\n",
    "\n",
    "            # Prepare the data to be stored in MongoDB\n",
    "            detected_object = {\n",
    "                \"filename\": filename,\n",
    "                \"label\": detected_label,\n",
    "                \"confidence\": float(confidence),\n",
    "                \"box\": [int(xmin), int(ymin), int(xmax), int(ymax)],\n",
    "                \"timestamp\": datetime.datetime.utcnow()\n",
    "            }\n",
    "\n",
    "            # Insert the data into MongoDB\n",
    "            collection.insert_one(detected_object)\n",
    "            print(f\"Image: {filename}, Detected {detected_label} with confidence {confidence:.4f} within box {box}\")\n",
    "\n",
    "        # Optionally, display the image with detected bounding boxes and labels\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off')\n",
    "        ax = plt.gca()\n",
    "        for obj in detected_objects:\n",
    "            xmin, ymin, xmax, ymax = obj['box']\n",
    "            detected_label = obj['label']\n",
    "            confidence = obj['confidence']\n",
    "            rect = plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, fill=False, color='red', linewidth=2)\n",
    "            ax.add_patch(rect)\n",
    "            plt.text(xmin, ymin, f'{detected_label} {confidence:.2f}', bbox=dict(facecolor='yellow', alpha=0.5))\n",
    "\n",
    "        plt.show()\n"
   ],
   "id": "5910c623ca2dfa2e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Configuration\n",
    "keyframe_dir = 'keyframes/'\n",
    "db_name = 'video_features_db'\n",
    "collection_name = 'features'\n",
    "batch_size = 32\n",
    "yolo_model_path = 'yolov5s.pt'  # Using the smallest version of YOLOv5 for demonstration\n",
    "\n",
    "# Configuration Google Colab\n",
    "# keyframe_dir = '/content/drive/MyDrive/keyframes'\n",
    "# db_name = 'video_features_db'\n",
    "# collection_name = 'features'\n",
    "# batch_size = 32\n",
    "# yolo_model_path = '/content/drive/MyDrive/yolov5s.pt'  # Using the smallest version of YOLOv5 for demonstration\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Initialize MongoDB client\n",
    "client = MongoClient('localhost', 27017)\n",
    "db = client[db_name]\n",
    "collection = db[collection_name]\n",
    "\n",
    "# Initialize pre-trained models\n",
    "vgg_model = VGG16(weights='imagenet', include_top=False, pooling='avg')\n",
    "resnet_model = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n",
    "\n",
    "# Load YOLOv5 model\n",
    "yolo_model = torch.hub.load('ultralytics/yolov5', 'custom', path=yolo_model_path)\n",
    "\n",
    "def extract_features(model, preprocess_input, img):\n",
    "    try:\n",
    "        img = cv2.resize(img, (224, 224))\n",
    "        img = img.astype('float32')\n",
    "        img = preprocess_input(img)\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        features = model.predict(img)\n",
    "        return features.flatten()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting features: {e}\")\n",
    "        return None\n",
    "\n",
    "def detect_objects_yolo(img):\n",
    "    try:\n",
    "        results = yolo_model(img)\n",
    "        detected_objects = results.pandas().xyxy[0].to_dict(orient=\"records\")\n",
    "        return detected_objects\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error detecting objects with YOLO: {e}\")\n",
    "        return []\n",
    "\n",
    "def process_keyframes(keyframe_directory, model, preprocess_input, model_name):\n",
    "    try:\n",
    "        for root, _, files in os.walk(keyframe_directory):\n",
    "            for file in tqdm(files, desc=f\"Extracting features using {model_name}\", unit=\"frame\"):\n",
    "                if file.endswith('.jpg'):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    img = cv2.imread(file_path)\n",
    "                    \n",
    "                    # YOLO Object Detection\n",
    "                    objects = detect_objects_yolo(img)\n",
    "                    \n",
    "                    # CNN Feature Extraction\n",
    "                    features = extract_features(model, preprocess_input, img)\n",
    "                    \n",
    "                    if features is not None:\n",
    "                        video_id, frame_id = os.path.basename(root), os.path.splitext(file)[0]\n",
    "                        feature_data = {\n",
    "                            'video_id': video_id,\n",
    "                            'frame_id': frame_id,\n",
    "                            'model': model_name,\n",
    "                            'features': features.tolist(),\n",
    "                            'objects': objects\n",
    "                        }\n",
    "                        collection.insert_one(feature_data)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing keyframes: {e}\")\n",
    "\n",
    "def process_videos(keyframe_directory):\n",
    "    process_keyframes(keyframe_directory, vgg_model, preprocess_input_vgg, 'VGG16')\n",
    "    process_keyframes(keyframe_directory, resnet_model, preprocess_input_resnet, 'ResNet50')\n",
    "\n",
    "logging.info(\"Starting feature extraction with YOLOv5 integration...\")\n",
    "process_videos(keyframe_dir)\n",
    "logging.info(\"Feature extraction with YOLOv5 integration completed successfully.\")\n"
   ],
   "id": "49fe172e86dd68c9",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
