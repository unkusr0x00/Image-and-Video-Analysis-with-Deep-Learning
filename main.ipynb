{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Imports",
   "id": "32d736dd469ee957"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import logging\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from pymongo import MongoClient\n",
    "from scenedetect import VideoManager, SceneManager\n",
    "from scenedetect.detectors import ContentDetector, ThresholdDetector\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input as preprocess_input_resnet\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input as preprocess_input_vgg\n",
    "from tqdm import tqdm\n",
    "\n"
   ],
   "id": "3918caaa9649f9c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Video Pre-processing",
   "id": "e22e91d1016abf2e"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Configuration\n",
    "input_dir = 'V3C1-100/'\n",
    "output_dir = 'preprocessed_videos/'\n",
    "output_format = 'mp4'\n",
    "\n",
    "# Configuration Google Colab\n",
    "# input_dir = '/content/drive/MyDrive/V3C1-100'\n",
    "# output_dir = '/content/drive/MyDrive/preprocessed_videos'\n",
    "# output_format = 'mp4'\n",
    "\n",
    "resize_width = 640\n",
    "resize_height = 480\n",
    "convert_to_grayscale = False\n",
    "frame_rate = 24  # Target frame rate\n",
    "max_workers = 1\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def preprocess_video(input_path, output_path, resize_dim, grayscale, frame_rate):\n",
    "    try:\n",
    "        print(f\"Processing video: {input_path}\")\n",
    "        cap = cv2.VideoCapture(input_path)\n",
    "        if not cap.isOpened():\n",
    "            raise ValueError(f\"Failed to open video file: {input_path}\")\n",
    "\n",
    "        original_frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "        if original_frame_rate == 0:\n",
    "            raise ValueError(f\"Failed to get frame rate for video file: {input_path}\")\n",
    "\n",
    "        frame_interval = int(original_frame_rate // frame_rate)\n",
    "\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(output_path, fourcc, frame_rate, resize_dim, not grayscale)\n",
    "\n",
    "        frame_count = 0\n",
    "        prev_gray = None\n",
    "\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            if frame_count % frame_interval == 0:\n",
    "                # Resize frame\n",
    "                frame = cv2.resize(frame, resize_dim, interpolation=cv2.INTER_AREA)\n",
    "                # # Convert to grayscale if needed for optical flow\n",
    "                # gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "                # # Apply noise reduction (computationally expensive)\n",
    "                # frame = cv2.fastNlMeansDenoisingColored(frame, None, 10, 10, 7, 21)\n",
    "                # # Apply histogram equalization\n",
    "                # if grayscale:\n",
    "                #     frame = cv2.equalizeHist(frame)\n",
    "                # else:\n",
    "                #     for i in range(3):\n",
    "                #         frame[:, :, i] = cv2.equalizeHist(frame[:, :, i])\n",
    "                # # Edge detection\n",
    "                # edges = cv2.Canny(frame, 100, 200)\n",
    "\n",
    "                # # Optical flow calculation\n",
    "                # if prev_gray is not None:\n",
    "                #     flow = cv2.calcOpticalFlowFarneback(prev_gray, gray_frame, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "                #     mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "                #     hsv = np.zeros_like(frame)\n",
    "                #     hsv[..., 1] = 255\n",
    "                #     hsv[..., 0] = ang * 180 / np.pi / 2\n",
    "                #     hsv[..., 2] = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX)\n",
    "                #     optical_flow = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "                #     frame = cv2.addWeighted(frame, 0.5, optical_flow, 0.5, 0)\n",
    "                # prev_gray = gray_frame\n",
    "\n",
    "                # # Feature extraction using HOG\n",
    "                # hog = cv2.HOGDescriptor()\n",
    "                # hog_features = hog.compute(frame)\n",
    "\n",
    "                # # Keypoint descriptors (ORB example)\n",
    "                # orb = cv2.ORB_create()\n",
    "                # kp, des = orb.detectAndCompute(frame, None)\n",
    "                # frame = cv2.drawKeypoints(frame, kp, None, color=(0, 255, 0), flags=0)\n",
    "                \n",
    "                out.write(frame)\n",
    "            frame_count += 1\n",
    "\n",
    "        cap.release()\n",
    "        out.release()\n",
    "        print(f\"Successfully processed video: {input_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {input_path}: {e}\")\n",
    "\n",
    "def get_video_files(input_directory):\n",
    "    video_files = []\n",
    "    for root, _, files in os.walk(input_directory):\n",
    "        for file in files:\n",
    "            if file.endswith(('.mp4', '.avi', '.mov', '.mkv')):\n",
    "                video_files.append(os.path.join(root, file))\n",
    "    return video_files\n",
    "\n",
    "def process_videos(video_files, output_directory, resize_dim, grayscale, frame_rate):\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = []\n",
    "        with tqdm(total=len(video_files), desc=\"Processing Videos\", unit=\"video\") as pbar:\n",
    "            for video_file in video_files:\n",
    "                relative_path = os.path.relpath(video_file, input_dir)\n",
    "                output_file = os.path.join(output_directory, os.path.splitext(relative_path)[0] + '.' + output_format)\n",
    "                os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "                future = executor.submit(preprocess_video, video_file, output_file, resize_dim, grayscale, frame_rate)\n",
    "                futures.append(future)\n",
    "\n",
    "            for future in futures:\n",
    "                future.add_done_callback(lambda p: pbar.update())\n",
    "            for future in futures:\n",
    "                future.result()  # Wait for all threads to complete\n",
    "\n",
    "print(\"Starting video pre-processing...\")\n",
    "video_files = get_video_files(input_dir)\n",
    "print(f\"Found {len(video_files)} video files.\")\n",
    "resize_dim = (resize_width, resize_height)\n",
    "process_videos(video_files, output_dir, resize_dim, convert_to_grayscale, frame_rate)\n",
    "print(\"All videos processed successfully.\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Shot Boundary Detection",
   "id": "f756ce17872689ef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T15:26:56.896307Z",
     "start_time": "2024-06-16T14:56:43.247011Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Configuration\n",
    "input_dir = 'preprocessed_videos/'\n",
    "output_dir = 'shot_boundaries/'\n",
    "keyframe_dir = 'keyframes/'\n",
    "\n",
    "# Configuration Google Colab\n",
    "# input_dir = '/content/drive/MyDrive/preprocessed_videos'\n",
    "# output_dir = '/content/drive/MyDrive/shot_boundaries'\n",
    "# keyframe_dir = '/content/drive/MyDrive/keyframes'\n",
    "\n",
    "min_scene_length = 15  # Minimum length of a scene in frames\n",
    "threshold = 30.0  # Threshold for the ThresholdDetector\n",
    "min_scene_len = 2  # Minimum number of frames a scene should last\n",
    "hist_threshold = 0.4  # Threshold for histogram comparison\n",
    "\n",
    "# Ensure the output and keyframe directories exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(keyframe_dir, exist_ok=True)\n",
    "\n",
    "def calculate_histogram_difference(frame1, frame2):\n",
    "    hist1 = cv2.calcHist([frame1], [0], None, [256], [0, 256])\n",
    "    hist2 = cv2.calcHist([frame2], [0], None, [256], [0, 256])\n",
    "    cv2.normalize(hist1, hist1)\n",
    "    cv2.normalize(hist2, hist2)\n",
    "    return cv2.compareHist(hist1, hist2, cv2.HISTCMP_CORREL)\n",
    "\n",
    "def detect_shot_boundaries(video_path, output_path, keyframe_path):\n",
    "    video_manager = VideoManager([video_path])\n",
    "    scene_manager = SceneManager()\n",
    "\n",
    "    # Add ContentDetector and ThresholdDetector\n",
    "    scene_manager.add_detector(ContentDetector(threshold=30.0, min_scene_len=min_scene_length))\n",
    "    scene_manager.add_detector(ThresholdDetector(threshold=threshold, min_scene_len=min_scene_len))\n",
    "\n",
    "    video_manager.set_downscale_factor()\n",
    "    video_manager.start()\n",
    "    scene_manager.detect_scenes(frame_source=video_manager)\n",
    "    scenes = scene_manager.get_scene_list()\n",
    "    print(f\"Detected {len(scenes)} scenes in video {video_path}\")\n",
    "\n",
    "    # Additional processing for gradual transitions\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    prev_frame = None\n",
    "    prev_gray = None\n",
    "    frame_num = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        if prev_frame is not None:\n",
    "            hist_diff = calculate_histogram_difference(prev_frame, frame)\n",
    "            if hist_diff < hist_threshold:\n",
    "                # Gradual transition detected\n",
    "                scenes.append((frame_num, frame_num + min_scene_len))\n",
    "            # Motion analysis using optical flow\n",
    "            if prev_gray is not None:\n",
    "                flow = cv2.calcOpticalFlowFarneback(prev_gray, gray_frame, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "                mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "                motion_magnitude = np.mean(mag)\n",
    "                if motion_magnitude > threshold:\n",
    "                    scenes.append((frame_num, frame_num + min_scene_len))\n",
    "        prev_frame = frame\n",
    "        prev_gray = gray_frame\n",
    "        frame_num += 1\n",
    "    cap.release()\n",
    "\n",
    "    # Remove duplicate and sort scenes\n",
    "    scenes = sorted(list(set(scenes)))\n",
    "    print(f\"Total scenes after processing: {len(scenes)}\")\n",
    "\n",
    "    # Save shot boundaries to a file\n",
    "    with open(output_path, 'w') as f:\n",
    "        for start_time, end_time in scenes:\n",
    "            f.write(f\"{start_time}, {end_time}\\n\")\n",
    "            # f.write(f\"{start_time.get_seconds()}, {end_time.get_seconds()}\\n\")\n",
    "            # f.write(f\"{start_time.get_frames()}, {end_time.get_frames()}\\n\")\n",
    "        print(f\"Shot boundaries saved to {output_path}\")\n",
    "\n",
    "    # Extract keyframes for each detected scene\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    for start, end in scenes:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, int(start))\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            keyframe_filename = os.path.join(keyframe_path, f\"{os.path.basename(video_path)}_start_{start}.jpg\")\n",
    "            cv2.imwrite(keyframe_filename, frame)\n",
    "    cap.release()\n",
    "    print(f\"Keyframes saved to {keyframe_path}\")\n",
    "\n",
    "\n",
    "def process_videos(video_files, output_directory, keyframe_directory):\n",
    "    for video_file in tqdm(video_files, desc=\"Detecting Shot Boundaries\", unit=\"video\"):\n",
    "        output_file = os.path.join(output_directory, os.path.splitext(os.path.basename(video_file))[0] + '_shots.txt')\n",
    "        keyframe_path = os.path.join(keyframe_directory, os.path.splitext(os.path.basename(video_file))[0])\n",
    "        os.makedirs(keyframe_path, exist_ok=True)\n",
    "        try:\n",
    "            detect_shot_boundaries(video_file, output_file, keyframe_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {video_file}: {e}\")\n",
    "\n",
    "def get_video_files(input_directory):\n",
    "    video_files = []\n",
    "    for root, _, files in os.walk(input_directory):\n",
    "        for file in files:\n",
    "            if file.endswith(('.mp4', '.avi', '.mov', '.mkv')):\n",
    "                video_files.append(os.path.join(root, file))\n",
    "    return video_files\n",
    "\n",
    "print(\"Starting shot boundary detection...\")\n",
    "video_files = get_video_files(input_dir)\n",
    "print(f\"Found {len(video_files)} video files to process.\")\n",
    "print(\"Video files:\", video_files)\n",
    "process_videos(video_files, output_dir, keyframe_dir)\n",
    "print(\"Shot boundary detection completed successfully.\")"
   ],
   "id": "81d3ae9c904bc3e2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting shot boundary detection...\n",
      "Found 97 video files to process.\n",
      "Video files: ['preprocessed_videos/00120/00120.mp4', 'preprocessed_videos/00118/00118.mp4', 'preprocessed_videos/00127/00127.mp4', 'preprocessed_videos/00111/00111.mp4', 'preprocessed_videos/00129/00129.mp4', 'preprocessed_videos/00116/00116.mp4', 'preprocessed_videos/00142/00142.mp4', 'preprocessed_videos/00189/00189.mp4', 'preprocessed_videos/00145/00145.mp4', 'preprocessed_videos/00173/00173.mp4', 'preprocessed_videos/00187/00187.mp4', 'preprocessed_videos/00180/00180.mp4', 'preprocessed_videos/00174/00174.mp4', 'preprocessed_videos/00128/00128.mp4', 'preprocessed_videos/00117/00117.mp4', 'preprocessed_videos/00110/00110.mp4', 'preprocessed_videos/00119/00119.mp4', 'preprocessed_videos/00126/00126.mp4', 'preprocessed_videos/00121/00121.mp4', 'preprocessed_videos/00175/00175.mp4', 'preprocessed_videos/00181/00181.mp4', 'preprocessed_videos/00186/00186.mp4', 'preprocessed_videos/00172/00172.mp4', 'preprocessed_videos/00144/00144.mp4', 'preprocessed_videos/00143/00143.mp4', 'preprocessed_videos/00188/00188.mp4', 'preprocessed_videos/00161/00161.mp4', 'preprocessed_videos/00195/00195.mp4', 'preprocessed_videos/00159/00159.mp4', 'preprocessed_videos/00168/00168.mp4', 'preprocessed_videos/00157/00157.mp4', 'preprocessed_videos/00103/00103.mp4', 'preprocessed_videos/00104/00104.mp4', 'preprocessed_videos/00132/00132.mp4', 'preprocessed_videos/00135/00135.mp4', 'preprocessed_videos/00169/00169.mp4', 'preprocessed_videos/00156/00156.mp4', 'preprocessed_videos/00151/00151.mp4', 'preprocessed_videos/00158/00158.mp4', 'preprocessed_videos/00167/00167.mp4', 'preprocessed_videos/00193/00193.mp4', 'preprocessed_videos/00194/00194.mp4', 'preprocessed_videos/00160/00160.mp4', 'preprocessed_videos/00134/00134.mp4', 'preprocessed_videos/00133/00133.mp4', 'preprocessed_videos/00105/00105.mp4', 'preprocessed_videos/00102/00102.mp4', 'preprocessed_videos/00146/00146.mp4', 'preprocessed_videos/00179/00179.mp4', 'preprocessed_videos/00141/00141.mp4', 'preprocessed_videos/00183/00183.mp4', 'preprocessed_videos/00177/00177.mp4', 'preprocessed_videos/00148/00148.mp4', 'preprocessed_videos/00170/00170.mp4', 'preprocessed_videos/00184/00184.mp4', 'preprocessed_videos/00124/00124.mp4', 'preprocessed_videos/00123/00123.mp4', 'preprocessed_videos/00115/00115.mp4', 'preprocessed_videos/00112/00112.mp4', 'preprocessed_videos/00185/00185.mp4', 'preprocessed_videos/00171/00171.mp4', 'preprocessed_videos/00176/00176.mp4', 'preprocessed_videos/00182/00182.mp4', 'preprocessed_videos/00149/00149.mp4', 'preprocessed_videos/00140/00140.mp4', 'preprocessed_videos/00147/00147.mp4', 'preprocessed_videos/00178/00178.mp4', 'preprocessed_videos/00113/00113.mp4', 'preprocessed_videos/00114/00114.mp4', 'preprocessed_videos/00122/00122.mp4', 'preprocessed_videos/00125/00125.mp4', 'preprocessed_videos/00107/00107.mp4', 'preprocessed_videos/00138/00138.mp4', 'preprocessed_videos/00100/00100.mp4', 'preprocessed_videos/00136/00136.mp4', 'preprocessed_videos/00109/00109.mp4', 'preprocessed_videos/00131/00131.mp4', 'preprocessed_videos/00191/00191.mp4', 'preprocessed_videos/00165/00165.mp4', 'preprocessed_videos/00162/00162.mp4', 'preprocessed_videos/00196/00196.mp4', 'preprocessed_videos/00154/00154.mp4', 'preprocessed_videos/00198/00198.mp4', 'preprocessed_videos/00153/00153.mp4', 'preprocessed_videos/00130/00130.mp4', 'preprocessed_videos/00137/00137.mp4', 'preprocessed_videos/00108/00108.mp4', 'preprocessed_videos/00101/00101.mp4', 'preprocessed_videos/00106/00106.mp4', 'preprocessed_videos/00139/00139.mp4', 'preprocessed_videos/00199/00199.mp4', 'preprocessed_videos/00152/00152.mp4', 'preprocessed_videos/00155/00155.mp4', 'preprocessed_videos/00197/00197.mp4', 'preprocessed_videos/00163/00163.mp4', 'preprocessed_videos/00164/00164.mp4', 'preprocessed_videos/00190/00190.mp4']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting Shot Boundaries:   0%|          | 0/97 [00:00<?, ?video/s]VideoManager is deprecated and will be removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 122 scenes in video preprocessed_videos/00120/00120.mp4\n",
      "Total scenes after processing: 168\n",
      "Shot boundaries saved to shot_boundaries/00120_shots.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting Shot Boundaries:   1%|          | 1/97 [19:49<31:42:42, 1189.20s/video]VideoManager is deprecated and will be removed.\n",
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x7ff58d4e4b00] moov atom not found\n",
      "OpenCV: Couldn't read video stream from file \"preprocessed_videos/00118/00118.mp4\"\n",
      "VideoManager is deprecated and will be removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyframes saved to keyframes/00120\n",
      "Error processing preprocessed_videos/00118/00118.mp4: [('00118.mp4', 'preprocessed_videos/00118/00118.mp4')]\n",
      "Detected 155 scenes in video preprocessed_videos/00127/00127.mp4\n",
      "Total scenes after processing: 232\n",
      "Shot boundaries saved to shot_boundaries/00127_shots.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting Shot Boundaries:   3%|▎         | 3/97 [27:32<12:32:04, 480.04s/video] VideoManager is deprecated and will be removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyframes saved to keyframes/00127\n",
      "Detected 164 scenes in video preprocessed_videos/00111/00111.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting Shot Boundaries:   3%|▎         | 3/97 [30:13<15:47:05, 604.52s/video]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 114\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFound \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(video_files)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m video files to process.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mVideo files:\u001B[39m\u001B[38;5;124m\"\u001B[39m, video_files)\n\u001B[0;32m--> 114\u001B[0m \u001B[43mprocess_videos\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvideo_files\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkeyframe_dir\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    115\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mShot boundary detection completed successfully.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[7], line 98\u001B[0m, in \u001B[0;36mprocess_videos\u001B[0;34m(video_files, output_directory, keyframe_directory)\u001B[0m\n\u001B[1;32m     96\u001B[0m os\u001B[38;5;241m.\u001B[39mmakedirs(keyframe_path, exist_ok\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     97\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 98\u001B[0m     \u001B[43mdetect_shot_boundaries\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvideo_file\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_file\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkeyframe_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     99\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    100\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mError processing \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mvideo_file\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[7], line 58\u001B[0m, in \u001B[0;36mdetect_shot_boundaries\u001B[0;34m(video_path, output_path, keyframe_path)\u001B[0m\n\u001B[1;32m     56\u001B[0m \u001B[38;5;66;03m# Motion analysis using optical flow\u001B[39;00m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m prev_gray \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m---> 58\u001B[0m     flow \u001B[38;5;241m=\u001B[39m \u001B[43mcv2\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcalcOpticalFlowFarneback\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprev_gray\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgray_frame\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0.5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m15\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     59\u001B[0m     mag, ang \u001B[38;5;241m=\u001B[39m cv2\u001B[38;5;241m.\u001B[39mcartToPolar(flow[\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m, \u001B[38;5;241m0\u001B[39m], flow[\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m, \u001B[38;5;241m1\u001B[39m])\n\u001B[1;32m     60\u001B[0m     motion_magnitude \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mmean(mag)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Feature Extraction",
   "id": "edb9ac3c1e364162"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Configuration\n",
    "keyframe_dir = 'keyframes/'\n",
    "db_name = 'video_features_db'\n",
    "collection_name = 'features'\n",
    "batch_size = 32\n",
    "yolo_model_path = 'yolov5s.pt'  # Using the smallest version of YOLOv5 for demonstration\n",
    "\n",
    "# Configuration Google Colab\n",
    "# keyframe_dir = '/content/drive/MyDrive/keyframes'\n",
    "# db_name = 'video_features_db'\n",
    "# collection_name = 'features'\n",
    "# batch_size = 32\n",
    "# yolo_model_path = '/content/drive/MyDrive/yolov5s.pt'  # Using the smallest version of YOLOv5 for demonstration\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Initialize MongoDB client\n",
    "client = MongoClient('localhost', 27017)\n",
    "db = client[db_name]\n",
    "collection = db[collection_name]\n",
    "\n",
    "# Initialize pre-trained models\n",
    "vgg_model = VGG16(weights='imagenet', include_top=False, pooling='avg')\n",
    "resnet_model = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n",
    "\n",
    "# Load YOLOv5 model\n",
    "yolo_model = torch.hub.load('ultralytics/yolov5', 'custom', path=yolo_model_path)\n",
    "\n",
    "def extract_features(model, preprocess_input, img):\n",
    "    try:\n",
    "        img = cv2.resize(img, (224, 224))\n",
    "        img = img.astype('float32')\n",
    "        img = preprocess_input(img)\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        features = model.predict(img)\n",
    "        return features.flatten()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting features: {e}\")\n",
    "        return None\n",
    "\n",
    "def detect_objects_yolo(img):\n",
    "    try:\n",
    "        results = yolo_model(img)\n",
    "        detected_objects = results.pandas().xyxy[0].to_dict(orient=\"records\")\n",
    "        return detected_objects\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error detecting objects with YOLO: {e}\")\n",
    "        return []\n",
    "\n",
    "def process_keyframes(keyframe_directory, model, preprocess_input, model_name):\n",
    "    try:\n",
    "        for root, _, files in os.walk(keyframe_directory):\n",
    "            for file in tqdm(files, desc=f\"Extracting features using {model_name}\", unit=\"frame\"):\n",
    "                if file.endswith('.jpg'):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    img = cv2.imread(file_path)\n",
    "                    \n",
    "                    # YOLO Object Detection\n",
    "                    objects = detect_objects_yolo(img)\n",
    "                    \n",
    "                    # CNN Feature Extraction\n",
    "                    features = extract_features(model, preprocess_input, img)\n",
    "                    \n",
    "                    if features is not None:\n",
    "                        video_id, frame_id = os.path.basename(root), os.path.splitext(file)[0]\n",
    "                        feature_data = {\n",
    "                            'video_id': video_id,\n",
    "                            'frame_id': frame_id,\n",
    "                            'model': model_name,\n",
    "                            'features': features.tolist(),\n",
    "                            'objects': objects\n",
    "                        }\n",
    "                        collection.insert_one(feature_data)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing keyframes: {e}\")\n",
    "\n",
    "def process_videos(keyframe_directory):\n",
    "    process_keyframes(keyframe_directory, vgg_model, preprocess_input_vgg, 'VGG16')\n",
    "    process_keyframes(keyframe_directory, resnet_model, preprocess_input_resnet, 'ResNet50')\n",
    "\n",
    "logging.info(\"Starting feature extraction with YOLOv5 integration...\")\n",
    "process_videos(keyframe_dir)\n",
    "logging.info(\"Feature extraction with YOLOv5 integration completed successfully.\")\n"
   ],
   "id": "49fe172e86dd68c9",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
