{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Feature Extraction using YOLOv9",
   "id": "d116d9bb1084af66"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import re\n",
    "\n",
    "def run_yolov9_detect(model, base_dir, video):\n",
    "    \"\"\"Run the yolov9 detect.py script with specified parameters.\"\"\"\n",
    "    command = [\n",
    "        'python3', 'yolov9-main/detect.py', \n",
    "        '--source', f'{os.path.join(base_dir, video)}', \n",
    "        '--img', '640', \n",
    "        '--weights', f'yolov9-main/yolov9-{model}-converted.pt', \n",
    "        '--name', f'{video}_{model}', \n",
    "        '--save-txt', \n",
    "        '--save-conf', \n",
    "        '--save-crop',\n",
    "        '--nosave'\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        result = subprocess.run(command, check=True, capture_output=True, text=True)\n",
    "        print(f\"Script output:\\n{result.stdout}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Script failed with error:\\n{e.stderr}\")\n",
    "        \n",
    "base_dir = 'keyframes_'\n",
    "models = ['c', 'e']\n",
    "# Create a list of directories in the keyframes folder\n",
    "directories = [f for f in os.listdir(base_dir)]\n",
    "directories.remove('.DS_Store')\n",
    "print(directories)\n",
    "\n",
    "# Run detection for each video and model\n",
    "for video in directories:\n",
    "    for model in models:\n",
    "        run_yolov9_detect(model, base_dir, video)\n"
   ],
   "id": "6234a86f42605f4b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Image Analysis and Description using Detectron2 and Vision-Transformer Models\n",
    "\n",
    "### 1. Object Detection and Segmentation\n",
    "- **Detectron2**: A  object detection library by Facebook AI Research (FAIR).\n",
    "  - **Model**: Utilizes a pre-trained Mask R-CNN model for detecting and segmenting objects within an image.\n",
    "  - **Configuration**: The model can be configured to run on either CPU or GPU, depending on the availability of a CUDA-enabled GPU.\n",
    "\n",
    "### 2. Image Captioning\n",
    "- **Transformers Library**: A  library by Hugging Face for natural language processing and vision tasks.\n",
    "  - **Model**: Uses the `VisionEncoderDecoderModel`, which combines a vision transformer (ViT) encoder with a GPT-2 decoder.\n",
    "  - **Processor**: The `ViTImageProcessor` for preprocessing images and `AutoTokenizer` for handling the text generation.\n",
    "\n",
    "\n",
    "1. **Image Loading**:\n",
    "   - Load an image using the `PIL` library.\n",
    "\n",
    "2. **Object Detection and Segmentation**:\n",
    "   - Configure and use the Detectron2 model to detect objects and generate segmentation masks.\n",
    "   - Visualize the segmentation results and extract instances (detected objects) from the image.\n",
    "\n",
    "3. **Image and Object Description**:\n",
    "   - Generate a detailed description of the entire image, focusing on the background.\n",
    "   - For each detected object, generate specific descriptions by focusing on their bounding boxes.\n",
    "\n",
    "#### Results\n",
    "\n",
    "- **Background Description**: Provides a textual description of the overall background of the image.\n",
    "- **Object Descriptions**: Generates individual descriptions for each detected object, detailing their appearance, actions, or other relevant attributes.\n"
   ],
   "id": "33a2429cdde6654d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog\n",
    "\n",
    "# Function to load and preprocess the image\n",
    "def load_image(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    return image\n",
    "\n",
    "# Function to detect objects in the image\n",
    "def detect_objects(image_path, device):\n",
    "    # Configure Detectron2\n",
    "    cfg = get_cfg()\n",
    "    cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
    "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model\n",
    "    cfg.MODEL.DEVICE = device  # Use CPU or GPU\n",
    "    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
    "    predictor = DefaultPredictor(cfg)\n",
    "\n",
    "    # Read the image\n",
    "    image = cv2.imread(image_path)\n",
    "    outputs = predictor(image)\n",
    "\n",
    "    # Visualize the predictions\n",
    "    v = Visualizer(image[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
    "    out = v.draw_instance_predictions(outputs[\"instances\"].to(device))\n",
    "    segmented_image = out.get_image()[:, :, ::-1]\n",
    "\n",
    "    return outputs[\"instances\"], segmented_image\n",
    "\n",
    "# Function to describe the background or specific object\n",
    "def describe_image(image, focus_area=None, device='cpu'):\n",
    "    # Load the model and processor\n",
    "    model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\").to(device)\n",
    "    processor = ViTImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "\n",
    "    # Preprocess the image\n",
    "    if focus_area:\n",
    "        image = image.crop(focus_area)\n",
    "\n",
    "    pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "\n",
    "    # Generate the caption\n",
    "    output_ids = model.generate(pixel_values, max_length=50, num_beams=4, eos_token_id=tokenizer.eos_token_id)\n",
    "    caption = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    return caption\n",
    "\n",
    "# Example usage\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "image_path = 'keyframes/00176/00176_Scene-5.jpg'\n",
    "\n",
    "# Load and detect objects in the image\n",
    "image = load_image(image_path)\n",
    "instances, segmented_image = detect_objects(image_path, device)\n",
    "\n",
    "# Describe the entire image (background)\n",
    "background_description = describe_image(image, device=device)\n",
    "print(\"Background description:\", background_description)\n",
    "\n",
    "# Describe specific objects\n",
    "# for i in range(len(instances)):\n",
    "#     bbox = instances.pred_boxes[i].tensor.cpu().numpy()[0]\n",
    "#     focus_area = (bbox[0], bbox[1], bbox[2], bbox[3])\n",
    "#     object_description = describe_image(image, focus_area, device=device)\n",
    "#     print(f\"Object {i+1} description:\", object_description)\n",
    "\n",
    "# Optionally, save the segmented image\n",
    "# segmented_image_pil = Image.fromarray(segmented_image)\n",
    "# segmented_image_pil.save(\"segmented_image.jpg\")\n"
   ],
   "id": "f11126eafb3a5e8b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Color Detection and/or Keyframe Description using BLIP",
   "id": "4fa558feac0a09f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "import torch\n",
    "import cv2\n",
    "import re\n",
    "\n",
    "# Load the BLIP model and processor\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "def generate_caption(image_path, prompt=\"Describe the colors.\"):\n",
    "    raw_image = cv2.imread(image_path)\n",
    "    raw_image = cv2.cvtColor(raw_image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Prepare image and prompt for BLIP\n",
    "    inputs = processor(images=raw_image, return_tensors=\"pt\")\n",
    "    \n",
    "    # Generate caption\n",
    "    out = model.generate(**inputs)\n",
    "    caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "    return caption\n",
    "\n",
    "def extract_colors(caption):\n",
    "    colors = [\"red\", \"green\", \"blue\", \"yellow\", \"white\", \"black\", \"orange\", \"purple\", \"brown\", \"gray\", \"pink\"]\n",
    "    found_colors = [color for color in colors if re.search(r'\\b' + color + r'\\b', caption)]\n",
    "    return found_colors\n",
    "\n",
    "\n",
    "# Load your cropped image\n",
    "image_path = 'yolov9-main/runs/detect/yolov9_640_detect3/crops/car/07.jpg'\n",
    "target_label = 'backpack'  # Replace with the target label (e.g., 'boat', 'backpack')\n",
    "\n",
    "# For description of keyframes use the corresponding path\n",
    "# image_path = 'keyframes/00101/00101.mp4_start_6894.jpg'  # Replace with the path to your image\n",
    "\n",
    "# Generate caption for the image\n",
    "caption = generate_caption(image_path)\n",
    "print(\"Generated Caption:\", caption)\n",
    "\n",
    "# Extract colors from the caption. Comment this out if keyframe description is needed\n",
    "colors_in_caption = extract_colors(caption)\n",
    "print(\"Colors found in caption:\", colors_in_caption)\n"
   ],
   "id": "ab90975d96a53539",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Dominant Color Detection with Enhanced Vibrance and Saturation\n",
    "\n",
    "This script processes an image to identify the two most dominant colors, emphasizing vibrant colors over less vibrant ones like gray and black. The key steps involved in the process are:\n",
    "\n",
    "1. **Image Preprocessing**:\n",
    "   - **Saturation Adjustment**: Increases the saturation of all colors in the image to make colors more vivid.\n",
    "   - **Vibrance Adjustment**: Boosts the vibrance of colors, particularly those that are less saturated, ensuring more nuanced color enhancement.\n",
    "\n",
    "2. **Color Filtering**:\n",
    "   - Converts the image to the HSV color space to filter out low-saturation colors, reducing the likelihood of selecting colors like gray or black as dominant colors.\n",
    "\n",
    "3. **K-Means Clustering**:\n",
    "   - Uses K-means clustering to identify the most common colors in the preprocessed image. The algorithm ensures that the top colors are distinct by checking the Euclidean distance between colors.\n",
    "\n",
    "4. **Color Preference Weighting**:\n",
    "   - Applies a weighting mechanism to prioritize vibrant colors (e.g., red, green, blue, yellow, orange) over less vibrant ones during the color matching process.\n",
    "\n",
    "5. **Output**:\n",
    "   - Ensures that the two most dominant colors are unique and outputs their names and RGB values. The original and preprocessed images are displayed side-by-side to visualize the effect of the adjustments."
   ],
   "id": "1c73ef5a57c74d1f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "import webcolors\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# Define a dictionary of common colors with an additional preference weight for each color\n",
    "COMMON_COLORS = {\n",
    "    'red': ('#FF0000', 10),\n",
    "    'green': ('#008000', 10),\n",
    "    'blue': ('#0000FF', 10),\n",
    "    'yellow': ('#FFFF00', 10),\n",
    "    'black': ('#000000', 1),\n",
    "    'white': ('#FFFFFF', 10),\n",
    "    # 'grey': ('#808080', 1),\n",
    "    'orange': ('#FFA500', 8)\n",
    "    # 'pink': ('#FFC0CB', 5),\n",
    "    # 'purple': ('#800080', 5),\n",
    "    # 'brown': ('#A52A2A', 3)\n",
    "}\n",
    "\n",
    "def closest_color(requested_color):\n",
    "    min_colors = {}\n",
    "    for name, (hex_code, weight) in COMMON_COLORS.items():\n",
    "        r_c, g_c, b_c = webcolors.hex_to_rgb(hex_code)\n",
    "        rd = (r_c - requested_color[0]) ** 2\n",
    "        gd = (g_c - requested_color[1]) ** 2\n",
    "        bd = (b_c - requested_color[2]) ** 2\n",
    "        min_colors[(rd + gd + bd) / weight] = name\n",
    "    return min_colors[min(min_colors.keys())]\n",
    "\n",
    "def adjust_saturation(image, saturation_scale=2.0):\n",
    "    \"\"\"\n",
    "    Adjust the saturation of the image.\n",
    "    \"\"\"\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV).astype(np.float32)\n",
    "    hsv[..., 1] *= saturation_scale\n",
    "    hsv[..., 1] = np.clip(hsv[..., 1], 0, 255)\n",
    "    return cv2.cvtColor(hsv.astype(np.uint8), cv2.COLOR_HSV2RGB)\n",
    "\n",
    "def adjust_vibrance(image, vibrance_scale=2.0):\n",
    "    \"\"\"\n",
    "    Adjust the vibrance of the image.\n",
    "    \"\"\"\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV).astype(np.float32)\n",
    "    saturation = hsv[..., 1]\n",
    "    mean_saturation = np.mean(saturation)\n",
    "    increase = (1 - (saturation / 255.0)) * (saturation - mean_saturation) * vibrance_scale\n",
    "    hsv[..., 1] = np.clip(saturation + increase, 0, 255)\n",
    "    return cv2.cvtColor(hsv.astype(np.uint8), cv2.COLOR_HSV2RGB)\n",
    "\n",
    "def preprocess_image(image):\n",
    "    # Adjust saturation\n",
    "    image = adjust_saturation(image, saturation_scale=3.0)\n",
    "    \n",
    "    # Adjust vibrance\n",
    "    image = adjust_vibrance(image, vibrance_scale=3.0)\n",
    "    \n",
    "    return image\n",
    "\n",
    "def get_dominant_colors(image_path, k=10, top_n=2, min_distance=50, saturation_threshold=50):\n",
    "    # Load image and convert to RGB\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Preprocess the image\n",
    "    preprocessed_image = preprocess_image(image)\n",
    "    \n",
    "    # Reshape the image to be a list of pixels\n",
    "    pixels = preprocessed_image.reshape((-1, 3))\n",
    "    \n",
    "    # Convert to HSV to filter by saturation\n",
    "    hsv_pixels = cv2.cvtColor(pixels.reshape(-1, 1, 3).astype(np.uint8), cv2.COLOR_RGB2HSV).reshape(-1, 3)\n",
    "    pixels = pixels[hsv_pixels[:, 1] > saturation_threshold]\n",
    "\n",
    "    # Perform K-means clustering to find the most common colors\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(pixels)\n",
    "    \n",
    "    # Find the most dominant clusters\n",
    "    counts = np.bincount(kmeans.labels_)\n",
    "    dominant_indices = np.argsort(-counts)\n",
    "    dominant_colors = kmeans.cluster_centers_[dominant_indices]\n",
    "    \n",
    "    # Ensure the top N colors are distinct and vibrant\n",
    "    distinct_colors = []\n",
    "    for color in dominant_colors:\n",
    "        if len(distinct_colors) == 0:\n",
    "            distinct_colors.append(color)\n",
    "        else:\n",
    "            if all(cdist([color], [distinct_color], metric='euclidean')[0][0] > min_distance for distinct_color in distinct_colors):\n",
    "                distinct_colors.append(color)\n",
    "            if len(distinct_colors) >= top_n:\n",
    "                break\n",
    "    \n",
    "    # Convert the dominant colors to human-readable names\n",
    "    dominant_color_names = [closest_color(color) for color in distinct_colors]\n",
    "    \n",
    "    # Ensure no duplicates in the top N colors\n",
    "    distinct_color_names = []\n",
    "    distinct_colors_filtered = []\n",
    "    for name, color in zip(dominant_color_names, distinct_colors):\n",
    "        if name not in distinct_color_names:\n",
    "            distinct_color_names.append(name)\n",
    "            distinct_colors_filtered.append(color)\n",
    "    \n",
    "    # If we don't have enough distinct colors, pick more from the list ensuring uniqueness\n",
    "    if len(distinct_color_names) < top_n:\n",
    "        for color in dominant_colors[len(distinct_colors):]:\n",
    "            name = closest_color(color)\n",
    "            if name not in distinct_color_names:\n",
    "                distinct_color_names.append(name)\n",
    "                distinct_colors_filtered.append(color)\n",
    "            if len(distinct_color_names) >= top_n:\n",
    "                break\n",
    "    \n",
    "    return distinct_color_names[:top_n], distinct_colors_filtered[:top_n], preprocessed_image\n",
    "\n",
    "# Load your cropped image\n",
    "image_path = 'yolov9-main/runs/detect/00120_e/crops/backpack/00120_Scene-562.jpg'\n",
    "\n",
    "# Get the dominant colors in the image\n",
    "dominant_color_names, dominant_colors, preprocessed_image = get_dominant_colors(image_path)\n",
    "print(\"Dominant Color Names:\", dominant_color_names)\n",
    "print(\"Dominant Colors RGB:\", dominant_colors)\n",
    "\n",
    "# Display the original image and the image with increased vibrance and saturation\n",
    "original_image = cv2.imread(image_path)\n",
    "original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(original_image)\n",
    "plt.title(\"Original Image\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(preprocessed_image)\n",
    "plt.title(f\"Preprocessed Image\\n(Dominant Colors: {dominant_color_names})\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()\n"
   ],
   "id": "a57d5aee833f1a0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# OCR using EasyOCR",
   "id": "a14a917f3197e26"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import easyocr\n",
    "import difflib\n",
    "import json\n",
    "\n",
    "def ocr_video(video_path, output_file, similarity_threshold=0.5, frame_step=10):\n",
    "    # Initialize the video capture and OCR reader\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    reader = easyocr.Reader(['en'])\n",
    "\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    # Initialize variables to store the results\n",
    "    results = []\n",
    "\n",
    "    prev_text = \"\"\n",
    "    current_block = None\n",
    "\n",
    "    for i in range(0, frame_count, frame_step):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Skip frames until we reach the next frame of interest\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "        \n",
    "        # Calculate the timestamp for the current frame\n",
    "        timestamp = i / fps\n",
    "\n",
    "        # Convert the frame to RGB (easyocr works on RGB images)\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Perform OCR on the frame\n",
    "        ocr_result_list = reader.readtext(frame_rgb, detail=0)\n",
    "        ocr_result = ' '.join(ocr_result_list).strip().lower()\n",
    "\n",
    "        # Skip frames with no text\n",
    "        if not ocr_result:\n",
    "            continue\n",
    "\n",
    "        # Calculate similarity with the previous text\n",
    "        similarity = difflib.SequenceMatcher(None, prev_text, ocr_result).ratio()\n",
    "\n",
    "        if similarity >= similarity_threshold:\n",
    "            # If the text is similar enough, update the end time of the current block\n",
    "            if current_block:\n",
    "                current_block['end_time'] = timestamp + (frame_step / fps)\n",
    "            else:\n",
    "                current_block = {\n",
    "                    'text': ocr_result,\n",
    "                    'start_time': timestamp,\n",
    "                    'end_time': timestamp + (frame_step / fps)\n",
    "                }\n",
    "        else:\n",
    "            # If the text is different enough, finalize the current block and start a new one\n",
    "            if current_block:\n",
    "                results.append(current_block)\n",
    "            current_block = {\n",
    "                'text': ocr_result,\n",
    "                'start_time': timestamp,\n",
    "                'end_time': timestamp + (frame_step / fps)\n",
    "            }\n",
    "        prev_text = ocr_result\n",
    "\n",
    "    # Finalize the last block\n",
    "    if current_block:\n",
    "        results.append(current_block)\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    # Save results to a JSON file\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "video_path = 'preprocessed_videos/00102/00102.mp4'\n",
    "output_file = 'ocr_results.json'\n",
    "similarity_threshold = 0.5\n",
    "frame_step = 10\n",
    "ocr_results = ocr_video(video_path, output_file, similarity_threshold, frame_step)\n",
    "\n",
    "# for result in ocr_results:\n",
    "#     print(f\"Text: {result['text']}, Start Time: {result['start_time']:.2f}, End Time: {result['end_time']:.2f}\")\n"
   ],
   "id": "59bca590da301029",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Search for a string in OCR results",
   "id": "73b6a69390285bd2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "import difflib\n",
    "\n",
    "def compare_string_with_ocr_results(ocr_file, input_string, similarity_threshold=0.8):\n",
    "    # Load OCR results from the file\n",
    "    with open(ocr_file, 'r') as f:\n",
    "        ocr_results = json.load(f)\n",
    "\n",
    "    # Normalize the input string\n",
    "    input_string = input_string.strip().lower()\n",
    "\n",
    "    matching_results = []\n",
    "\n",
    "    for result in ocr_results:\n",
    "        ocr_text = result['text']\n",
    "        similarity = difflib.SequenceMatcher(None, ocr_text, input_string).ratio()\n",
    "\n",
    "        if similarity >= similarity_threshold:\n",
    "            matching_results.append({\n",
    "                'text': ocr_text,\n",
    "                'start_time': result['start_time'],\n",
    "                'end_time': result['end_time']\n",
    "            })\n",
    "\n",
    "    return matching_results\n",
    "\n",
    "# Example usage\n",
    "ocr_file = 'ocr_results.json'\n",
    "input_string = 'Get reliable diving gear'\n",
    "similarity_threshold = 0.6\n",
    "\n",
    "matching_results = compare_string_with_ocr_results(ocr_file, input_string, similarity_threshold)\n",
    "\n",
    "for result in matching_results:\n",
    "    print(f\"Matching Text: {result['text']}, Start Time: {result['start_time']:.2f}, End Time: {result['end_time']:.2f}\")\n"
   ],
   "id": "b14682c29ba71b28",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Using CLIP",
   "id": "dc708a090c0bb15"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from sklearn.metrics import confusion_matrix, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Load models and processors\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "faster_rcnn = fasterrcnn_resnet50_fpn(pretrained=True).to(device)\n",
    "faster_rcnn.eval()\n",
    "\n",
    "# Define the folder containing images\n",
    "frame_dir = '00110'\n",
    "annotation_dir = '00110'\n",
    "\n",
    "# Define the queries for evaluation\n",
    "queries = [\"A photo of a person\", \"A photo of a bird\", \"A photo of a truck\", \"A photo of a horse\", \"A photo of a car\"]\n",
    "query_labels = [\"person\", \"bird\", \"truck\", \"horse\", \"car\"]\n",
    "\n",
    "# Mapping to unify bus and train as truck\n",
    "class_mapping = {\n",
    "    \"bus\": \"truck\",\n",
    "    \"train\": \"truck\"\n",
    "}\n",
    "\n",
    "def get_ground_truth_labels(xml_file):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    labels = []\n",
    "    boxes = []\n",
    "    for member in root.findall('object'):\n",
    "        label = member.find('name').text\n",
    "        if label in class_mapping:\n",
    "            label = class_mapping[label]\n",
    "        if label in query_labels:\n",
    "            labels.append(label)\n",
    "            bndbox = member.find('bndbox')\n",
    "            xmin = int(bndbox.find('xmin').text)\n",
    "            ymin = int(bndbox.find('ymin').text)\n",
    "            xmax = int(bndbox.find('xmax').text)\n",
    "            ymax = int(bndbox.find('ymax').text)\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "    return labels, boxes\n",
    "\n",
    "def iou(box1, box2):\n",
    "    \"\"\"Calculate Intersection Over Union (IOU) of two bounding boxes.\"\"\"\n",
    "    x1, y1, x2, y2 = box1\n",
    "    x1_p, y1_p, x2_p, y2_p = box2\n",
    "\n",
    "    xi1 = max(x1, x1_p)\n",
    "    yi1 = max(y1, y1_p)\n",
    "    xi2 = min(x2, x2_p)\n",
    "    yi2 = min(y2, y2_p)\n",
    "    inter_area = max(0, xi2 - xi1) * max(0, yi2 - yi1)\n",
    "\n",
    "    box1_area = (x2 - x1) * (y2 - y1)\n",
    "    box2_area = (x2_p - x1_p) * (y2_p - y1_p)\n",
    "    union_area = box1_area + box2_area - inter_area\n",
    "\n",
    "    iou = inter_area / union_area\n",
    "    return iou\n",
    "\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "for frame_name in os.listdir(frame_dir):\n",
    "    if frame_name.endswith(\".jpg\"):\n",
    "        frame_path = os.path.join(frame_dir, frame_name)\n",
    "        annotation_path = os.path.join(annotation_dir, frame_name.replace(\".jpg\", \".xml\"))\n",
    "        \n",
    "        img = Image.open(frame_path).convert(\"RGB\")\n",
    "        if img is None:\n",
    "            print(f\"Error reading image: {frame_path}\")\n",
    "            continue\n",
    "        \n",
    "        # Get ground truth labels and boxes\n",
    "        if not os.path.exists(annotation_path):\n",
    "            print(f\"Annotation file not found: {annotation_path}\")\n",
    "            continue\n",
    "        gt_labels, gt_boxes = get_ground_truth_labels(annotation_path)\n",
    "\n",
    "        # Convert image to tensor\n",
    "        transform = T.Compose([T.ToTensor()])\n",
    "        img_tensor = transform(img).to(device)\n",
    "\n",
    "        # Generate bounding boxes using Faster R-CNN\n",
    "        with torch.no_grad():\n",
    "            predictions = faster_rcnn([img_tensor])\n",
    "        pred_boxes = predictions[0]['boxes'].cpu().numpy()\n",
    "        pred_scores = predictions[0]['scores'].cpu().numpy()\n",
    "\n",
    "        detected_labels = []\n",
    "        detected_boxes = []\n",
    "\n",
    "        for box, score in zip(pred_boxes, pred_scores):\n",
    "            if score > 0.5:  # Adjust threshold as needed\n",
    "                xmin, ymin, xmax, ymax = box\n",
    "                cropped_img = img.crop((xmin, ymin, xmax, ymax))\n",
    "                inputs = clip_processor(text=queries, images=cropped_img, return_tensors=\"pt\", padding=True).to(device)\n",
    "                outputs = clip_model(**inputs)\n",
    "                logits_per_image = outputs.logits_per_image.softmax(dim=1).detach().cpu().numpy().flatten()\n",
    "\n",
    "                best_idx = logits_per_image.argmax()\n",
    "                detected_label = query_labels[best_idx]\n",
    "                confidence = logits_per_image[best_idx]\n",
    "\n",
    "                detected_labels.append((detected_label, confidence, [xmin, ymin, xmax, ymax]))\n",
    "\n",
    "        # For visualization and evaluation\n",
    "        frame_true_labels = []\n",
    "        frame_predicted_labels = []\n",
    "        matched_predictions = [False] * len(detected_labels)\n",
    "\n",
    "        for label, gt_box in zip(gt_labels, gt_boxes):\n",
    "            frame_true_labels.append(label)\n",
    "            matched = False\n",
    "            for i, (detected_label, _, detected_box) in enumerate(detected_labels):\n",
    "                if iou(gt_box, detected_box) > 0.5 and not matched_predictions[i]:\n",
    "                    frame_predicted_labels.append(detected_label)\n",
    "                    matched_predictions[i] = True\n",
    "                    matched = True\n",
    "                    break\n",
    "            if not matched:\n",
    "                frame_predicted_labels.append(\"none\")\n",
    "\n",
    "        for i, (detected_label, _, detected_box) in enumerate(detected_labels):\n",
    "            if not matched_predictions[i]:\n",
    "                frame_true_labels.append(\"none\")\n",
    "                frame_predicted_labels.append(detected_label)\n",
    "\n",
    "        true_labels.extend(frame_true_labels)\n",
    "        predicted_labels.extend(frame_predicted_labels)\n",
    "\n",
    "# Print the true and predicted labels for debugging\n",
    "print(\"True Labels:\", true_labels)\n",
    "print(\"Predicted Labels:\", predicted_labels)\n",
    "\n",
    "# Ensure the lengths are equal\n",
    "min_length = min(len(true_labels), len(predicted_labels))\n",
    "filtered_true_labels = true_labels[:min_length]\n",
    "filtered_predicted_labels = predicted_labels[:min_length]\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(filtered_true_labels, filtered_predicted_labels, labels=query_labels + [\"none\"])\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Calculate recall for each class\n",
    "recall = recall_score(filtered_true_labels, filtered_predicted_labels, average=None, labels=query_labels)\n",
    "print(\"Recall for each class:\")\n",
    "print(recall)\n",
    "\n",
    "# Weighted average recall\n",
    "weighted_recall = recall_score(filtered_true_labels, filtered_predicted_labels, average='weighted', labels=query_labels)\n",
    "print(\"Weighted Recall:\")\n",
    "print(weighted_recall)\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", xticklabels=query_labels + [\"none\"], yticklabels=query_labels + [\"none\"], cmap=\"Blues\")\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ],
   "id": "61b73cb7cc657425",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Using CLIP and BLIP",
   "id": "a478096b113fa506"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel, BlipProcessor, BlipForConditionalGeneration\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models.detection import FasterRCNN, fasterrcnn_resnet50_fpn\n",
    "import matplotlib.pyplot as plt\n",
    "from pymongo import MongoClient, errors\n",
    "import datetime\n",
    "\n",
    "# Define the folder containing images\n",
    "folder_path = \"keyframes/00102\"\n",
    "\n",
    "# Define the paths to the weights\n",
    "fasterrcnn_weights_path = \"weights/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\"\n",
    "resnet50_weights_path = \"weights/resnet50-0676ba61.pth\"\n",
    "\n",
    "# Check if the files exist\n",
    "assert os.path.exists(fasterrcnn_weights_path), \"Faster R-CNN weights file not found!\"\n",
    "assert os.path.exists(resnet50_weights_path), \"ResNet50 weights file not found!\"\n",
    "\n",
    "# Print paths to verify\n",
    "print(f\"Faster R-CNN weights path: {fasterrcnn_weights_path}\")\n",
    "print(f\"ResNet50 weights path: {resnet50_weights_path}\")\n",
    "\n",
    "# Load the ResNet50 backbone with local weights\n",
    "from torchvision.models import resnet50\n",
    "backbone = resnet50(pretrained=False)\n",
    "backbone_state_dict = torch.load(resnet50_weights_path, map_location=torch.device('cpu'))\n",
    "\n",
    "# Remove the fully connected layer weights from the state dictionary\n",
    "backbone_state_dict.pop(\"fc.weight\", None)\n",
    "backbone_state_dict.pop(\"fc.bias\", None)\n",
    "\n",
    "# Load the state dictionary with strict=False to ignore missing keys\n",
    "backbone.load_state_dict(backbone_state_dict, strict=False)\n",
    "\n",
    "# Create a custom backbone with FPN from the loaded ResNet50 backbone\n",
    "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
    "\n",
    "# Use the backbone with FPN, ensuring it uses the locally loaded weights\n",
    "backbone_with_fpn = resnet_fpn_backbone('resnet50', pretrained=False, norm_layer=torch.nn.BatchNorm2d)\n",
    "backbone_with_fpn.body.load_state_dict(backbone.state_dict(), strict=False)\n",
    "\n",
    "# Load the Faster R-CNN model with the custom backbone\n",
    "detection_model = FasterRCNN(backbone=backbone_with_fpn, num_classes=91)  # Use the backbone explicitly\n",
    "detection_model.load_state_dict(torch.load(fasterrcnn_weights_path, map_location=torch.device('cpu')))\n",
    "detection_model.eval()\n",
    "\n",
    "# Load the CLIP model and processor\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Load the BLIP captioning model and processor\n",
    "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "# MongoDB setup with error handling\n",
    "try:\n",
    "    client = MongoClient('mongodb://localhost:27017/', serverSelectionTimeoutMS=5000)\n",
    "    client.server_info()  # Trigger exception if cannot connect to db\n",
    "    db = client['object_detection']\n",
    "    collection = db['detected_objects']\n",
    "except errors.ServerSelectionTimeoutError as err:\n",
    "    print(\"Failed to connect to MongoDB server:\", err)\n",
    "    exit(1)\n",
    "\n",
    "# Transform for the object detection model\n",
    "transform = T.Compose([T.ToTensor()])\n",
    "\n",
    "# Function to generate captions using BLIP\n",
    "def generate_caption(image):\n",
    "    inputs = blip_processor(images=image, return_tensors=\"pt\")\n",
    "    out = blip_model.generate(**inputs)\n",
    "    caption = blip_processor.decode(out[0], skip_special_tokens=True)\n",
    "    return caption\n",
    "\n",
    "# Process each image in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "        image_path = os.path.join(folder_path, filename)\n",
    "        image = Image.open(image_path)\n",
    "\n",
    "        # Transform image for the detection model\n",
    "        image_tensor = transform(image)\n",
    "\n",
    "        # Get bounding boxes\n",
    "        with torch.no_grad():\n",
    "            detections = detection_model([image_tensor])[0]\n",
    "\n",
    "        # Filter out low-confidence detections\n",
    "        threshold = 0.5\n",
    "        boxes = [box for box, score in zip(detections['boxes'], detections['scores']) if score > threshold]\n",
    "\n",
    "        detected_objects = []\n",
    "\n",
    "        # Use BLIP to generate captions for objects within bounding boxes\n",
    "        for box in boxes:\n",
    "            xmin, ymin, xmax, ymax = box.int().numpy()\n",
    "            cropped_image = image.crop((xmin, ymin, xmax, ymax))\n",
    "            caption = generate_caption(cropped_image)\n",
    "            inputs = clip_processor(text=[caption], images=cropped_image, return_tensors=\"pt\", padding=True)\n",
    "            outputs = clip_model(**inputs)\n",
    "            probs = outputs.logits_per_image.softmax(dim=1).detach().cpu().numpy()[0]\n",
    "            detected_label = caption  # Use the generated caption as the label\n",
    "            confidence = probs.max()\n",
    "\n",
    "            detected_objects.append({\n",
    "                \"box\": [xmin, ymin, xmax, ymax],\n",
    "                \"label\": detected_label,\n",
    "                \"confidence\": float(confidence)\n",
    "            })\n",
    "\n",
    "            # Prepare the data to be stored in MongoDB\n",
    "            detected_object = {\n",
    "                \"filename\": filename,\n",
    "                \"label\": detected_label,\n",
    "                \"confidence\": float(confidence),\n",
    "                \"box\": [int(xmin), int(ymin), int(xmax), int(ymax)],\n",
    "                \"timestamp\": datetime.datetime.utcnow()\n",
    "            }\n",
    "\n",
    "            # Insert the data into MongoDB\n",
    "            collection.insert_one(detected_object)\n",
    "            print(f\"Image: {filename}, Detected {detected_label} with confidence {confidence:.4f} within box {box}\")\n",
    "\n",
    "        # Optionally, display the image with detected bounding boxes and labels\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off')\n",
    "        ax = plt.gca()\n",
    "        for obj in detected_objects:\n",
    "            xmin, ymin, xmax, ymax = obj['box']\n",
    "            detected_label = obj['label']\n",
    "            confidence = obj['confidence']\n",
    "            rect = plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, fill=False, color='red', linewidth=2)\n",
    "            ax.add_patch(rect)\n",
    "            plt.text(xmin, ymin, f'{detected_label} {confidence:.2f}', bbox=dict(facecolor='yellow', alpha=0.5))\n",
    "\n",
    "        plt.show()\n"
   ],
   "id": "7583e65a08720710",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
