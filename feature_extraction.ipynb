{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Feature Extraction",
   "id": "dcd09d758e3d79c4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Using YOLOv9",
   "id": "d116d9bb1084af66"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import re\n",
    "\n",
    "def run_yolov9_detect(model, base_dir, video):\n",
    "    \"\"\"Run the yolov9 detect.py script with specified parameters.\"\"\"\n",
    "    command = [\n",
    "        'python3', 'yolov9-main/detect.py', \n",
    "        '--source', f'{os.path.join(base_dir, video)}', \n",
    "        '--img', '640', \n",
    "        '--weights', f'yolov9-main/yolov9-{model}-converted.pt', \n",
    "        '--name', f'{video}_{model}', \n",
    "        '--save-txt', \n",
    "        '--save-conf', \n",
    "        '--save-crop',\n",
    "        '--nosave'\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        result = subprocess.run(command, check=True, capture_output=True, text=True)\n",
    "        print(f\"Script output:\\n{result.stdout}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Script failed with error:\\n{e.stderr}\")\n",
    "        \n",
    "def merge_results(yolo_output_dir, folders, models):\n",
    "    merged_dir = 'object_detection_results'\n",
    "    os.makedirs(merged_dir, exist_ok=True)\n",
    "    \n",
    "    base_folders = set()\n",
    "    # Regular expression to match strings starting with 5 digits\n",
    "    pattern = re.compile(r'^\\d{5}')\n",
    "    for folder in folders:\n",
    "        match = pattern.match(folder)\n",
    "        if match:\n",
    "            prefix = match.group(0)\n",
    "            base_folders.add(prefix)\n",
    "    \n",
    "    for video in base_folders:\n",
    "        video_merged_dir = os.path.join(merged_dir, video)\n",
    "        os.makedirs(video_merged_dir, exist_ok=True)\n",
    "        \n",
    "        labels_merged_dir = os.path.join(video_merged_dir, 'labels')\n",
    "        crops_merged_dir = os.path.join(video_merged_dir, 'crops')\n",
    "        os.makedirs(labels_merged_dir, exist_ok=True)\n",
    "        os.makedirs(crops_merged_dir, exist_ok=True)\n",
    "\n",
    "        all_detections = {}\n",
    "\n",
    "        for model in models:\n",
    "            model_result_dir = os.path.join(yolo_output_dir, f'{video}_{model}')\n",
    "            model_labels_dir = os.path.join(model_result_dir, 'labels')\n",
    "            model_crops_dir = os.path.join(model_result_dir, 'crops')\n",
    "\n",
    "            if not os.path.exists(model_labels_dir) or not os.path.exists(model_crops_dir):\n",
    "                continue\n",
    "\n",
    "            # Process label files\n",
    "            for label_file in os.listdir(model_labels_dir):\n",
    "                if label_file.endswith('.txt'):\n",
    "                    label_path = os.path.join(model_labels_dir, label_file)\n",
    "                    with open(label_path, 'r') as f:\n",
    "                        lines = f.readlines()\n",
    "\n",
    "                    if label_file not in all_detections:\n",
    "                        all_detections[label_file] = []\n",
    "                    \n",
    "                    for line in lines:\n",
    "                        parts = line.strip().split()\n",
    "                        class_id = parts[0]\n",
    "                        if class_id not in all_detections[label_file]:\n",
    "                            all_detections[label_file].append((model, line))\n",
    "\n",
    "        # Merge and copy images\n",
    "        for label_file, detections in all_detections.items():\n",
    "            label_merged_path = os.path.join(labels_merged_dir, label_file)\n",
    "            with open(label_merged_path, 'w') as f_out:\n",
    "                for idx, (model, line) in enumerate(detections):\n",
    "                    parts = line.strip().split()\n",
    "                    class_id = parts[0]\n",
    "\n",
    "                    src_img_name = label_file.replace('.txt', '') + f\"{idx+1}.jpg\"\n",
    "                    src_img_path = os.path.join(base_dir, f'{video}_{model}', 'crops', class_id, src_img_name)\n",
    "                    \n",
    "                    new_img_name = f\"{os.path.splitext(label_file)[0]}_{model}_{idx+1}.jpg\"\n",
    "                    new_img_path = os.path.join(crops_merged_dir, class_id)\n",
    "                    os.makedirs(new_img_path, exist_ok=True)\n",
    "                    dst_img_path = os.path.join(new_img_path, new_img_name)\n",
    "\n",
    "                    if os.path.exists(src_img_path):\n",
    "                        shutil.copy(src_img_path, dst_img_path)\n",
    "                        parts = line.strip().split()\n",
    "                        f_out.write(' '.join(parts) + '\\n')\n",
    "        \n",
    "base_dir = 'keyframes_'\n",
    "models = ['c', 'e']\n",
    "# Create a list of directories in the keyframes folder\n",
    "directories = [f for f in os.listdir(base_dir)]\n",
    "directories.remove('.DS_Store')\n",
    "print(directories)\n",
    "\n",
    "# Run detection for each video and model\n",
    "# for video in directories:\n",
    "#     for model in models:\n",
    "#         run_yolov9_detect(model, base_dir, video)\n",
    "\n",
    "# Merge results\n",
    "yolo_output_dir = 'yolov9-main/runs/detect'\n",
    "folders = [f for f in os.listdir(yolo_output_dir)]\n",
    "folders.remove('.DS_Store')\n",
    "merge_results(yolo_output_dir, folders, models)"
   ],
   "id": "6234a86f42605f4b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Color Detection using BLIP",
   "id": "4fa558feac0a09f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "import torch\n",
    "import cv2\n",
    "import re\n",
    "\n",
    "# Load the BLIP model and processor\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "def generate_caption(image_path, prompt=\"Describe the colors.\"):\n",
    "    raw_image = cv2.imread(image_path)\n",
    "    raw_image = cv2.cvtColor(raw_image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Prepare image and prompt for BLIP\n",
    "    inputs = processor(images=raw_image, return_tensors=\"pt\")\n",
    "    \n",
    "    # Generate caption\n",
    "    out = model.generate(**inputs)\n",
    "    caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "    return caption\n",
    "\n",
    "def extract_colors(caption):\n",
    "    colors = [\"red\", \"green\", \"blue\", \"yellow\", \"white\", \"black\", \"orange\", \"purple\", \"brown\", \"gray\", \"pink\"]\n",
    "    found_colors = [color for color in colors if re.search(r'\\b' + color + r'\\b', caption)]\n",
    "    return found_colors\n",
    "\n",
    "\n",
    "# Load your cropped image\n",
    "image_path = 'yolov9-main/runs/detect/00120_e/crops/backpack/00120_Scene-77.jpg'  # Replace with the path to your image\n",
    "target_label = 'backpack'  # Replace with the target label (e.g., 'boat', 'backpack')\n",
    "\n",
    "# Generate caption for the image\n",
    "caption = generate_caption(image_path)\n",
    "print(\"Generated Caption:\", caption)\n",
    "\n",
    "# Extract colors from the caption\n",
    "colors_in_caption = extract_colors(caption)\n",
    "print(\"Colors found in caption:\", colors_in_caption)\n"
   ],
   "id": "ab90975d96a53539",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Color detection using three most dominant colors",
   "id": "1c73ef5a57c74d1f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Predefined list of colors and their RGB values\n",
    "COLOR_NAMES = {\n",
    "    \"red\": [255, 0, 0],\n",
    "    \"green\": [0, 255, 0],\n",
    "    \"blue\": [0, 0, 255],\n",
    "    \"yellow\": [255, 255, 0],\n",
    "    \"white\": [255, 255, 255],\n",
    "    \"black\": [0, 0, 0],\n",
    "    \"orange\": [255, 165, 0],\n",
    "    \"purple\": [128, 0, 128],\n",
    "    \"brown\": [165, 42, 42],\n",
    "    \"gray\": [128, 128, 128],\n",
    "    \"pink\": [255, 192, 203]\n",
    "}\n",
    "\n",
    "def closest_color_name(rgb_color):\n",
    "    # Calculate the Euclidean distance between the given color and all predefined colors\n",
    "    distances = {name: np.linalg.norm(np.array(rgb_color) - np.array(rgb_val)) for name, rgb_val in COLOR_NAMES.items()}\n",
    "    # Find the color with the smallest distance\n",
    "    closest_color = min(distances, key=distances.get)\n",
    "    return closest_color\n",
    "\n",
    "def get_dominant_colors(image_path, k=3):\n",
    "    # Read the image\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Reshape the image to be a list of pixels\n",
    "    pixels = image.reshape(-1, 3)\n",
    "\n",
    "    # Apply KMeans clustering to find the most dominant colors\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    kmeans.fit(pixels)\n",
    "\n",
    "    # Get the cluster centers (the dominant colors)\n",
    "    colors = kmeans.cluster_centers_\n",
    "\n",
    "    # Get the number of pixels assigned to each cluster\n",
    "    counts = np.bincount(kmeans.labels_)\n",
    "\n",
    "    # Get the percentage of each color\n",
    "    percentages = counts / sum(counts)\n",
    "\n",
    "    # Combine colors and percentages\n",
    "    dominant_colors = sorted(zip(colors, percentages), key=lambda x: -x[1])\n",
    "\n",
    "    # Map RGB colors to closest color names\n",
    "    named_dominant_colors = [(closest_color_name(color), percentage) for color, percentage in dominant_colors]\n",
    "\n",
    "    return named_dominant_colors\n",
    "\n",
    "def plot_colors(dominant_colors):\n",
    "    # Create a square showing the dominant colors\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 2),\n",
    "                           subplot_kw=dict(xticks=[], yticks=[], frame_on=False))\n",
    "\n",
    "    bar = np.zeros((50, 300, 3), dtype=np.uint8)\n",
    "    start = 0\n",
    "\n",
    "    for color_name, percentage in dominant_colors:\n",
    "        color_rgb = COLOR_NAMES[color_name]\n",
    "        end = start + int(percentage * bar.shape[1])\n",
    "        bar[:, start:end] = color_rgb\n",
    "        start = end\n",
    "\n",
    "    ax.imshow(bar)\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "image_path = 'yolov9-main/runs/detect/00120_e/crops/person/00120_Scene-273.jpg'  # Replace with the path to your image\n",
    "\n",
    "# Get the three most dominant colors\n",
    "dominant_colors = get_dominant_colors(image_path, k=3)\n",
    "\n",
    "# Print the colors and their dominance\n",
    "for color_name, percentage in dominant_colors:\n",
    "    print(f\"Color: {color_name}, Percentage: {percentage*100:.2f}%\")\n",
    "\n",
    "# Plot the dominant colors\n",
    "plot_colors(dominant_colors)\n"
   ],
   "id": "a57d5aee833f1a0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# OCR using EasyOCR",
   "id": "a14a917f3197e26"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import easyocr\n",
    "import difflib\n",
    "import json\n",
    "\n",
    "def ocr_video(video_path, output_file, similarity_threshold=0.8, frame_step=10):\n",
    "    # Initialize the video capture and OCR reader\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    reader = easyocr.Reader(['en'])\n",
    "\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    # Initialize variables to store the results\n",
    "    results = []\n",
    "\n",
    "    prev_text = \"\"\n",
    "    current_block = None\n",
    "\n",
    "    for i in range(0, frame_count, frame_step):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Skip frames until we reach the next frame of interest\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "        \n",
    "        # Calculate the timestamp for the current frame\n",
    "        timestamp = i / fps\n",
    "\n",
    "        # Convert the frame to RGB (easyocr works on RGB images)\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Perform OCR on the frame\n",
    "        ocr_result_list = reader.readtext(frame_rgb, detail=0)\n",
    "        ocr_result = ' '.join(ocr_result_list).strip().lower()\n",
    "\n",
    "        # Skip frames with no text\n",
    "        if not ocr_result:\n",
    "            continue\n",
    "\n",
    "        # Calculate similarity with the previous text\n",
    "        similarity = difflib.SequenceMatcher(None, prev_text, ocr_result).ratio()\n",
    "\n",
    "        if similarity >= similarity_threshold:\n",
    "            # If the text is similar enough, update the end time of the current block\n",
    "            if current_block:\n",
    "                current_block['end_time'] = timestamp + (frame_step / fps)\n",
    "            else:\n",
    "                current_block = {\n",
    "                    'text': ocr_result,\n",
    "                    'start_time': timestamp,\n",
    "                    'end_time': timestamp + (frame_step / fps)\n",
    "                }\n",
    "        else:\n",
    "            # If the text is different enough, finalize the current block and start a new one\n",
    "            if current_block:\n",
    "                results.append(current_block)\n",
    "            current_block = {\n",
    "                'text': ocr_result,\n",
    "                'start_time': timestamp,\n",
    "                'end_time': timestamp + (frame_step / fps)\n",
    "            }\n",
    "        prev_text = ocr_result\n",
    "\n",
    "    # Finalize the last block\n",
    "    if current_block:\n",
    "        results.append(current_block)\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    # Save results to a JSON file\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "video_path = 'preprocessed_videos/00102/00102.mp4'\n",
    "output_file = 'ocr_results.json'\n",
    "similarity_threshold = 0.5\n",
    "frame_step = 10\n",
    "ocr_results = ocr_video(video_path, output_file, similarity_threshold, frame_step)\n",
    "\n",
    "# for result in ocr_results:\n",
    "#     print(f\"Text: {result['text']}, Start Time: {result['start_time']:.2f}, End Time: {result['end_time']:.2f}\")\n"
   ],
   "id": "59bca590da301029",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Search for a string in OCR results",
   "id": "73b6a69390285bd2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "import difflib\n",
    "\n",
    "def compare_string_with_ocr_results(ocr_file, input_string, similarity_threshold=0.8):\n",
    "    # Load OCR results from the file\n",
    "    with open(ocr_file, 'r') as f:\n",
    "        ocr_results = json.load(f)\n",
    "\n",
    "    # Normalize the input string\n",
    "    input_string = input_string.strip().lower()\n",
    "\n",
    "    matching_results = []\n",
    "\n",
    "    for result in ocr_results:\n",
    "        ocr_text = result['text']\n",
    "        similarity = difflib.SequenceMatcher(None, ocr_text, input_string).ratio()\n",
    "\n",
    "        if similarity >= similarity_threshold:\n",
    "            matching_results.append({\n",
    "                'text': ocr_text,\n",
    "                'start_time': result['start_time'],\n",
    "                'end_time': result['end_time']\n",
    "            })\n",
    "\n",
    "    return matching_results\n",
    "\n",
    "# Example usage\n",
    "ocr_file = 'ocr_results.json'\n",
    "input_string = 'Get reliable diving gear'\n",
    "similarity_threshold = 0.6\n",
    "\n",
    "matching_results = compare_string_with_ocr_results(ocr_file, input_string, similarity_threshold)\n",
    "\n",
    "for result in matching_results:\n",
    "    print(f\"Matching Text: {result['text']}, Start Time: {result['start_time']:.2f}, End Time: {result['end_time']:.2f}\")\n"
   ],
   "id": "b14682c29ba71b28",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Using CLIP",
   "id": "dc708a090c0bb15"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from sklearn.metrics import confusion_matrix, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Load models and processors\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "faster_rcnn = fasterrcnn_resnet50_fpn(pretrained=True).to(device)\n",
    "faster_rcnn.eval()\n",
    "\n",
    "# Define the folder containing images\n",
    "frame_dir = '00110'\n",
    "annotation_dir = '00110'\n",
    "\n",
    "# Define the queries for evaluation\n",
    "queries = [\"A photo of a person\", \"A photo of a bird\", \"A photo of a truck\", \"A photo of a horse\", \"A photo of a car\"]\n",
    "query_labels = [\"person\", \"bird\", \"truck\", \"horse\", \"car\"]\n",
    "\n",
    "# Mapping to unify bus and train as truck\n",
    "class_mapping = {\n",
    "    \"bus\": \"truck\",\n",
    "    \"train\": \"truck\"\n",
    "}\n",
    "\n",
    "def get_ground_truth_labels(xml_file):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    labels = []\n",
    "    boxes = []\n",
    "    for member in root.findall('object'):\n",
    "        label = member.find('name').text\n",
    "        if label in class_mapping:\n",
    "            label = class_mapping[label]\n",
    "        if label in query_labels:\n",
    "            labels.append(label)\n",
    "            bndbox = member.find('bndbox')\n",
    "            xmin = int(bndbox.find('xmin').text)\n",
    "            ymin = int(bndbox.find('ymin').text)\n",
    "            xmax = int(bndbox.find('xmax').text)\n",
    "            ymax = int(bndbox.find('ymax').text)\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "    return labels, boxes\n",
    "\n",
    "def iou(box1, box2):\n",
    "    \"\"\"Calculate Intersection Over Union (IOU) of two bounding boxes.\"\"\"\n",
    "    x1, y1, x2, y2 = box1\n",
    "    x1_p, y1_p, x2_p, y2_p = box2\n",
    "\n",
    "    xi1 = max(x1, x1_p)\n",
    "    yi1 = max(y1, y1_p)\n",
    "    xi2 = min(x2, x2_p)\n",
    "    yi2 = min(y2, y2_p)\n",
    "    inter_area = max(0, xi2 - xi1) * max(0, yi2 - yi1)\n",
    "\n",
    "    box1_area = (x2 - x1) * (y2 - y1)\n",
    "    box2_area = (x2_p - x1_p) * (y2_p - y1_p)\n",
    "    union_area = box1_area + box2_area - inter_area\n",
    "\n",
    "    iou = inter_area / union_area\n",
    "    return iou\n",
    "\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "for frame_name in os.listdir(frame_dir):\n",
    "    if frame_name.endswith(\".jpg\"):\n",
    "        frame_path = os.path.join(frame_dir, frame_name)\n",
    "        annotation_path = os.path.join(annotation_dir, frame_name.replace(\".jpg\", \".xml\"))\n",
    "        \n",
    "        img = Image.open(frame_path).convert(\"RGB\")\n",
    "        if img is None:\n",
    "            print(f\"Error reading image: {frame_path}\")\n",
    "            continue\n",
    "        \n",
    "        # Get ground truth labels and boxes\n",
    "        if not os.path.exists(annotation_path):\n",
    "            print(f\"Annotation file not found: {annotation_path}\")\n",
    "            continue\n",
    "        gt_labels, gt_boxes = get_ground_truth_labels(annotation_path)\n",
    "\n",
    "        # Convert image to tensor\n",
    "        transform = T.Compose([T.ToTensor()])\n",
    "        img_tensor = transform(img).to(device)\n",
    "\n",
    "        # Generate bounding boxes using Faster R-CNN\n",
    "        with torch.no_grad():\n",
    "            predictions = faster_rcnn([img_tensor])\n",
    "        pred_boxes = predictions[0]['boxes'].cpu().numpy()\n",
    "        pred_scores = predictions[0]['scores'].cpu().numpy()\n",
    "\n",
    "        detected_labels = []\n",
    "        detected_boxes = []\n",
    "\n",
    "        for box, score in zip(pred_boxes, pred_scores):\n",
    "            if score > 0.5:  # Adjust threshold as needed\n",
    "                xmin, ymin, xmax, ymax = box\n",
    "                cropped_img = img.crop((xmin, ymin, xmax, ymax))\n",
    "                inputs = clip_processor(text=queries, images=cropped_img, return_tensors=\"pt\", padding=True).to(device)\n",
    "                outputs = clip_model(**inputs)\n",
    "                logits_per_image = outputs.logits_per_image.softmax(dim=1).detach().cpu().numpy().flatten()\n",
    "\n",
    "                best_idx = logits_per_image.argmax()\n",
    "                detected_label = query_labels[best_idx]\n",
    "                confidence = logits_per_image[best_idx]\n",
    "\n",
    "                detected_labels.append((detected_label, confidence, [xmin, ymin, xmax, ymax]))\n",
    "\n",
    "        # For visualization and evaluation\n",
    "        frame_true_labels = []\n",
    "        frame_predicted_labels = []\n",
    "        matched_predictions = [False] * len(detected_labels)\n",
    "\n",
    "        for label, gt_box in zip(gt_labels, gt_boxes):\n",
    "            frame_true_labels.append(label)\n",
    "            matched = False\n",
    "            for i, (detected_label, _, detected_box) in enumerate(detected_labels):\n",
    "                if iou(gt_box, detected_box) > 0.5 and not matched_predictions[i]:\n",
    "                    frame_predicted_labels.append(detected_label)\n",
    "                    matched_predictions[i] = True\n",
    "                    matched = True\n",
    "                    break\n",
    "            if not matched:\n",
    "                frame_predicted_labels.append(\"none\")\n",
    "\n",
    "        for i, (detected_label, _, detected_box) in enumerate(detected_labels):\n",
    "            if not matched_predictions[i]:\n",
    "                frame_true_labels.append(\"none\")\n",
    "                frame_predicted_labels.append(detected_label)\n",
    "\n",
    "        true_labels.extend(frame_true_labels)\n",
    "        predicted_labels.extend(frame_predicted_labels)\n",
    "\n",
    "# Print the true and predicted labels for debugging\n",
    "print(\"True Labels:\", true_labels)\n",
    "print(\"Predicted Labels:\", predicted_labels)\n",
    "\n",
    "# Ensure the lengths are equal\n",
    "min_length = min(len(true_labels), len(predicted_labels))\n",
    "filtered_true_labels = true_labels[:min_length]\n",
    "filtered_predicted_labels = predicted_labels[:min_length]\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(filtered_true_labels, filtered_predicted_labels, labels=query_labels + [\"none\"])\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Calculate recall for each class\n",
    "recall = recall_score(filtered_true_labels, filtered_predicted_labels, average=None, labels=query_labels)\n",
    "print(\"Recall for each class:\")\n",
    "print(recall)\n",
    "\n",
    "# Weighted average recall\n",
    "weighted_recall = recall_score(filtered_true_labels, filtered_predicted_labels, average='weighted', labels=query_labels)\n",
    "print(\"Weighted Recall:\")\n",
    "print(weighted_recall)\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", xticklabels=query_labels + [\"none\"], yticklabels=query_labels + [\"none\"], cmap=\"Blues\")\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ],
   "id": "61b73cb7cc657425",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Using CLIP and BLIP",
   "id": "a478096b113fa506"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel, BlipProcessor, BlipForConditionalGeneration\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models.detection import FasterRCNN, fasterrcnn_resnet50_fpn\n",
    "import matplotlib.pyplot as plt\n",
    "from pymongo import MongoClient, errors\n",
    "import datetime\n",
    "\n",
    "# Define the folder containing images\n",
    "folder_path = \"keyframes/00102\"\n",
    "\n",
    "# Define the paths to the weights\n",
    "fasterrcnn_weights_path = \"weights/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\"\n",
    "resnet50_weights_path = \"weights/resnet50-0676ba61.pth\"\n",
    "\n",
    "# Check if the files exist\n",
    "assert os.path.exists(fasterrcnn_weights_path), \"Faster R-CNN weights file not found!\"\n",
    "assert os.path.exists(resnet50_weights_path), \"ResNet50 weights file not found!\"\n",
    "\n",
    "# Print paths to verify\n",
    "print(f\"Faster R-CNN weights path: {fasterrcnn_weights_path}\")\n",
    "print(f\"ResNet50 weights path: {resnet50_weights_path}\")\n",
    "\n",
    "# Load the ResNet50 backbone with local weights\n",
    "from torchvision.models import resnet50\n",
    "backbone = resnet50(pretrained=False)\n",
    "backbone_state_dict = torch.load(resnet50_weights_path, map_location=torch.device('cpu'))\n",
    "\n",
    "# Remove the fully connected layer weights from the state dictionary\n",
    "backbone_state_dict.pop(\"fc.weight\", None)\n",
    "backbone_state_dict.pop(\"fc.bias\", None)\n",
    "\n",
    "# Load the state dictionary with strict=False to ignore missing keys\n",
    "backbone.load_state_dict(backbone_state_dict, strict=False)\n",
    "\n",
    "# Create a custom backbone with FPN from the loaded ResNet50 backbone\n",
    "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
    "\n",
    "# Use the backbone with FPN, ensuring it uses the locally loaded weights\n",
    "backbone_with_fpn = resnet_fpn_backbone('resnet50', pretrained=False, norm_layer=torch.nn.BatchNorm2d)\n",
    "backbone_with_fpn.body.load_state_dict(backbone.state_dict(), strict=False)\n",
    "\n",
    "# Load the Faster R-CNN model with the custom backbone\n",
    "detection_model = FasterRCNN(backbone=backbone_with_fpn, num_classes=91)  # Use the backbone explicitly\n",
    "detection_model.load_state_dict(torch.load(fasterrcnn_weights_path, map_location=torch.device('cpu')))\n",
    "detection_model.eval()\n",
    "\n",
    "# Load the CLIP model and processor\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Load the BLIP captioning model and processor\n",
    "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "# MongoDB setup with error handling\n",
    "try:\n",
    "    client = MongoClient('mongodb://localhost:27017/', serverSelectionTimeoutMS=5000)\n",
    "    client.server_info()  # Trigger exception if cannot connect to db\n",
    "    db = client['object_detection']\n",
    "    collection = db['detected_objects']\n",
    "except errors.ServerSelectionTimeoutError as err:\n",
    "    print(\"Failed to connect to MongoDB server:\", err)\n",
    "    exit(1)\n",
    "\n",
    "# Transform for the object detection model\n",
    "transform = T.Compose([T.ToTensor()])\n",
    "\n",
    "# Function to generate captions using BLIP\n",
    "def generate_caption(image):\n",
    "    inputs = blip_processor(images=image, return_tensors=\"pt\")\n",
    "    out = blip_model.generate(**inputs)\n",
    "    caption = blip_processor.decode(out[0], skip_special_tokens=True)\n",
    "    return caption\n",
    "\n",
    "# Process each image in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "        image_path = os.path.join(folder_path, filename)\n",
    "        image = Image.open(image_path)\n",
    "\n",
    "        # Transform image for the detection model\n",
    "        image_tensor = transform(image)\n",
    "\n",
    "        # Get bounding boxes\n",
    "        with torch.no_grad():\n",
    "            detections = detection_model([image_tensor])[0]\n",
    "\n",
    "        # Filter out low-confidence detections\n",
    "        threshold = 0.5\n",
    "        boxes = [box for box, score in zip(detections['boxes'], detections['scores']) if score > threshold]\n",
    "\n",
    "        detected_objects = []\n",
    "\n",
    "        # Use BLIP to generate captions for objects within bounding boxes\n",
    "        for box in boxes:\n",
    "            xmin, ymin, xmax, ymax = box.int().numpy()\n",
    "            cropped_image = image.crop((xmin, ymin, xmax, ymax))\n",
    "            caption = generate_caption(cropped_image)\n",
    "            inputs = clip_processor(text=[caption], images=cropped_image, return_tensors=\"pt\", padding=True)\n",
    "            outputs = clip_model(**inputs)\n",
    "            probs = outputs.logits_per_image.softmax(dim=1).detach().cpu().numpy()[0]\n",
    "            detected_label = caption  # Use the generated caption as the label\n",
    "            confidence = probs.max()\n",
    "\n",
    "            detected_objects.append({\n",
    "                \"box\": [xmin, ymin, xmax, ymax],\n",
    "                \"label\": detected_label,\n",
    "                \"confidence\": float(confidence)\n",
    "            })\n",
    "\n",
    "            # Prepare the data to be stored in MongoDB\n",
    "            detected_object = {\n",
    "                \"filename\": filename,\n",
    "                \"label\": detected_label,\n",
    "                \"confidence\": float(confidence),\n",
    "                \"box\": [int(xmin), int(ymin), int(xmax), int(ymax)],\n",
    "                \"timestamp\": datetime.datetime.utcnow()\n",
    "            }\n",
    "\n",
    "            # Insert the data into MongoDB\n",
    "            collection.insert_one(detected_object)\n",
    "            print(f\"Image: {filename}, Detected {detected_label} with confidence {confidence:.4f} within box {box}\")\n",
    "\n",
    "        # Optionally, display the image with detected bounding boxes and labels\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off')\n",
    "        ax = plt.gca()\n",
    "        for obj in detected_objects:\n",
    "            xmin, ymin, xmax, ymax = obj['box']\n",
    "            detected_label = obj['label']\n",
    "            confidence = obj['confidence']\n",
    "            rect = plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, fill=False, color='red', linewidth=2)\n",
    "            ax.add_patch(rect)\n",
    "            plt.text(xmin, ymin, f'{detected_label} {confidence:.2f}', bbox=dict(facecolor='yellow', alpha=0.5))\n",
    "\n",
    "        plt.show()\n"
   ],
   "id": "7583e65a08720710",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
