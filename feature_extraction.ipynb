{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## YOLOv9 Detection Script\n",
    "\n",
    "This script automates the process of running object detection using YOLOv9 on a collection of video files. The key components and their functionalities are as follows:\n",
    "\n",
    "- **Dependencies**: The script imports essential modules (`os` and `subprocess`) for directory management, command execution, and string operations.\n",
    "- **`run_yolov9_detect` Function**: This function constructs and executes a command to run the `detect.py` script from the YOLOv9 repository. It specifies parameters such as the source video file, image size, weights, and options for saving detection results.\n",
    "- **Base Directory and Models**: The base directory (`keyframes`) contains the video files, and the `models` list specifies the versions of YOLOv9 to be used (`c` and `e`).\n",
    "- **Directory Listing**: The script lists all directories in the base directory, excluding system files like `.DS_Store`.\n",
    "- **Detection Execution**: The script iterates through each video file and model, running the detection function for each combination, and prints the directories being processed.\n",
    "\n",
    "This setup ensures that YOLOv9 detection is run systematically across all specified videos and models, with the results and any errors being output to the console.\n"
   ],
   "id": "d116d9bb1084af66"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import re\n",
    "\n",
    "def run_yolov9_detect(model, base_dir, video):\n",
    "    \"\"\"\n",
    "    Run the yolov9 detect.py script with specified parameters.\n",
    "\n",
    "    Args:\n",
    "        model (str): The YOLOv9 model to use ('c' or 'e').\n",
    "        base_dir (str): Base directory containing the videos.\n",
    "        video (str): The video file or directory to run detection on.\n",
    "    \"\"\"\n",
    "    command = [\n",
    "        'python3', 'yolov9-main/detect.py', \n",
    "        '--source', f'{os.path.join(base_dir, video)}', \n",
    "        '--img', '640', \n",
    "        '--weights', f'yolov9-main/yolov9-{model}-converted.pt', \n",
    "        '--name', f'{video}_{model}', \n",
    "        '--save-txt', \n",
    "        '--save-conf', \n",
    "        '--save-crop',\n",
    "        '--nosave'\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        result = subprocess.run(command, check=True, capture_output=True, text=True)\n",
    "        print(f\"Script output:\\n{result.stdout}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Script failed with error:\\n{e.stderr}\")\n",
    "\n",
    "# Configuration\n",
    "base_dir = 'keyframes'  # Directory containing the keyframes\n",
    "models = ['c', 'e']  # List of YOLOv9 models to use\n",
    "\n",
    "# Create a list of directories in the keyframes folder\n",
    "directories = [f for f in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, f))]\n",
    "if '.DS_Store' in directories:\n",
    "    directories.remove('.DS_Store')\n",
    "print(directories)\n",
    "\n",
    "# Run detection for each video and model\n",
    "for video in directories:\n",
    "    for model in models:\n",
    "        run_yolov9_detect(model, base_dir, video)\n"
   ],
   "id": "6234a86f42605f4b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Image Analysis and Description using Detectron2 and Vision-Transformer Models\n",
    "\n",
    "### 1. Object Detection and Segmentation\n",
    "- **Detectron2**: A  object detection library by Facebook AI Research (FAIR).\n",
    "  - **Model**: Utilizes a pre-trained Mask R-CNN model for detecting and segmenting objects within an image.\n",
    "  - **Configuration**: The model can be configured to run on either CPU or GPU, depending on the availability of a CUDA-enabled GPU.\n",
    "\n",
    "### 2. Image Captioning\n",
    "- **Transformers Library**: A  library by Hugging Face for natural language processing and vision tasks.\n",
    "  - **Model**: Uses the `VisionEncoderDecoderModel`, which combines a vision transformer (ViT) encoder with a GPT-2 decoder.\n",
    "  - **Processor**: The `ViTImageProcessor` for preprocessing images and `AutoTokenizer` for handling the text generation.\n",
    "\n",
    "\n",
    "1. **Image Loading**:\n",
    "   - Load an image using the `PIL` library.\n",
    "\n",
    "2. **Object Detection and Segmentation**:\n",
    "   - Configure and use the Detectron2 model to detect objects and generate segmentation masks.\n",
    "   - Visualize the segmentation results and extract instances (detected objects) from the image.\n",
    "\n",
    "3. **Image and Object Description**:\n",
    "   - Generate a detailed description of the entire image.\n",
    "   - For each detected object, generate specific descriptions by focusing on their bounding boxes.\n",
    "\n",
    "#### Results\n",
    "\n",
    "- **Description**: Provides a textual description of the overall image.\n",
    "- **Object Descriptions**: Generates individual descriptions for each detected object, detailing their appearance, actions, or other relevant attributes.\n"
   ],
   "id": "33a2429cdde6654d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog\n",
    "\n",
    "# Function to load and preprocess the image\n",
    "def load_image(image_path):\n",
    "    \"\"\"\n",
    "    Load an image from the specified file path.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the image file.\n",
    "\n",
    "    Returns:\n",
    "        PIL.Image.Image: Loaded image.\n",
    "    \"\"\"\n",
    "    image = Image.open(image_path)\n",
    "    return image\n",
    "\n",
    "# Function to detect objects in the image\n",
    "def detect_objects(image_path, device):\n",
    "    \"\"\"\n",
    "    Detect objects in an image using Detectron2 and visualize the results.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the image file.\n",
    "        device (str): Device to run the model on ('cpu' or 'cuda').\n",
    "\n",
    "    Returns:\n",
    "        detectron2.structures.Instances: Detected instances.\n",
    "        np.ndarray: Image with visualized instance predictions.\n",
    "    \"\"\"\n",
    "    # Configure Detectron2\n",
    "    cfg = get_cfg()\n",
    "    cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
    "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model\n",
    "    cfg.MODEL.DEVICE = device  # Use CPU or GPU\n",
    "    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
    "    predictor = DefaultPredictor(cfg)\n",
    "\n",
    "    # Read the image\n",
    "    image = cv2.imread(image_path)\n",
    "    outputs = predictor(image)\n",
    "\n",
    "    # Visualize the predictions\n",
    "    v = Visualizer(image[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
    "    out = v.draw_instance_predictions(outputs[\"instances\"].to(device))\n",
    "    segmented_image = out.get_image()[:, :, ::-1]\n",
    "\n",
    "    return outputs[\"instances\"], segmented_image\n",
    "\n",
    "# Function to describe the background or specific object\n",
    "def describe_image(image, focus_area=None, device='cpu'):\n",
    "    \"\"\"\n",
    "    Generate a description for the entire image or a specific object using a vision-language model.\n",
    "\n",
    "    Args:\n",
    "        image (PIL.Image.Image): Image to describe.\n",
    "        focus_area (tuple, optional): Bounding box (left, upper, right, lower) to crop the image. Defaults to None.\n",
    "        device (str): Device to run the model on ('cpu' or 'cuda').\n",
    "\n",
    "    Returns:\n",
    "        str: Generated description of the image or object.\n",
    "    \"\"\"\n",
    "    # Load the model and processor\n",
    "    model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\").to(device)\n",
    "    processor = ViTImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "\n",
    "    # Preprocess the image\n",
    "    if focus_area:\n",
    "        image = image.crop(focus_area)\n",
    "\n",
    "    pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "\n",
    "    # Generate the caption\n",
    "    output_ids = model.generate(pixel_values, max_length=100, num_beams=4, eos_token_id=tokenizer.eos_token_id)\n",
    "    caption = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    return caption\n",
    "\n",
    "# Example usage\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "image_path = 'keyframes/00176/00176_Scene-5.jpg'\n",
    "\n",
    "# Load and detect objects in the image\n",
    "image = load_image(image_path)\n",
    "instances, segmented_image = detect_objects(image_path, device)\n",
    "\n",
    "# Describe the entire image (background)\n",
    "image_description = describe_image(image, device=device)\n",
    "print(\"Image description:\", image_description)\n",
    "\n",
    "# Describe specific objects (uncomment to use)\n",
    "# for i in range(len(instances)):\n",
    "#     bbox = instances.pred_boxes[i].tensor.cpu().numpy()[0]\n",
    "#     focus_area = (bbox[0], bbox[1], bbox[2], bbox[3])\n",
    "#     object_description = describe_image(image, focus_area, device=device)\n",
    "#     print(f\"Object {i+1} description:\", object_description)\n",
    "\n",
    "# Optionally, save the segmented image (uncomment to use)\n",
    "# segmented_image_pil = Image.fromarray(segmented_image)\n",
    "# segmented_image_pil.save(\"segmented_image.jpg\")\n"
   ],
   "id": "f11126eafb3a5e8b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Color Detection and Caption Generation with BLIP\n",
    "\n",
    "This script uses the BLIP (Bootstrapping Language-Image Pre-training) model to generate captions for images extracted from video frames and detect colors mentioned in those captions. The process involves several key steps:\n",
    "\n",
    "1. **Model Initialization**:\n",
    "   - Loads the BLIP model (`Salesforce/blip-image-captioning-base`) and its corresponding processor to generate image captions based on the given prompt.\n",
    "\n",
    "2. **Caption Generation**:\n",
    "   - **Image Loading and Processing**: Reads and converts images to the appropriate format for BLIP.\n",
    "   - **Prompt-Based Captioning**: Generates descriptive captions for each image using the BLIP model, with a focus on color description.\n",
    "\n",
    "3. **Color Extraction**:\n",
    "   - Analyzes the generated captions to identify and extract color names from a predefined list (e.g., red, green, blue, yellow, etc.).\n",
    "\n",
    "4. **Directory Processing**:\n",
    "   - **Folder Traversal**: Walks through the specified directory structure to locate image files.\n",
    "   - **File Filtering**: Processes only relevant image files (e.g., `.jpg` and `.png`).\n",
    "\n",
    "5. **Result Structuring**:\n",
    "   - **Metadata Parsing**: Extracts video name, scene number, and object class from the file path to organize results.\n",
    "   - **Data Organization**: Stores the generated captions and detected colors in a nested dictionary structure for easy retrieval.\n",
    "\n",
    "6. **Output Generation**:\n",
    "   - **JSON Serialization**: Saves the results to JSON files, one per video, ensuring the output directory exists.\n",
    "   - **Progress Visualization**: Utilizes `tqdm` to display progress bars during the processing of files and directories.\n"
   ],
   "id": "4fa558feac0a09f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the BLIP model and processor\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "def generate_caption(image_path, prompt=\"Describe the colors.\"):\n",
    "    \"\"\"\n",
    "    Generate a caption for an image using the BLIP model.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the image file.\n",
    "        prompt (str): Prompt for the caption generation model. Default is \"Describe the colors.\".\n",
    "\n",
    "    Returns:\n",
    "        str: Generated caption.\n",
    "    \"\"\"\n",
    "    raw_image = cv2.imread(image_path)\n",
    "    raw_image = cv2.cvtColor(raw_image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Prepare image and prompt for BLIP\n",
    "    inputs = processor(images=raw_image, return_tensors=\"pt\")\n",
    "    \n",
    "    # Generate caption\n",
    "    out = model.generate(**inputs)\n",
    "    caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "    return caption\n",
    "\n",
    "def extract_colors(caption):\n",
    "    \"\"\"\n",
    "    Extract color names from a generated caption.\n",
    "\n",
    "    Args:\n",
    "        caption (str): Generated caption from the BLIP model.\n",
    "\n",
    "    Returns:\n",
    "        list: List of found colors in the caption.\n",
    "    \"\"\"\n",
    "    colors = [\"red\", \"green\", \"blue\", \"yellow\", \"white\", \"black\", \"orange\", \"purple\", \"brown\", \"gray\", \"pink\"]\n",
    "    found_colors = [color for color in colors if re.search(r'\\b' + color + r'\\b', caption)]\n",
    "    return found_colors\n",
    "\n",
    "def process_video_folder(video_folder):\n",
    "    \"\"\"\n",
    "    Process all image files in a video folder, generating captions and extracting colors.\n",
    "\n",
    "    Args:\n",
    "        video_folder (str): Path to the video folder containing image files.\n",
    "\n",
    "    Returns:\n",
    "        dict: Results organized by video name, scene number, and object class.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for root, dirs, files in os.walk(video_folder):\n",
    "        # Use tqdm to display a progress bar\n",
    "        for file in tqdm(files, desc=f\"Processing files in {video_folder}\", unit=\"file\"):\n",
    "            if file.endswith(\".jpg\") or file.endswith(\".png\"):\n",
    "                image_path = os.path.join(root, file)\n",
    "                caption = generate_caption(image_path)\n",
    "                colors_in_caption = extract_colors(caption)\n",
    "\n",
    "                # Parse video name, scene number, and object class from the file path\n",
    "                match = re.match(r'.*/(\\d{5})_Scene-(\\d+)', image_path)\n",
    "                if match:\n",
    "                    video_name = match.group(1)\n",
    "                    scene_number = match.group(2)\n",
    "                    object_class = os.path.basename(os.path.dirname(image_path))\n",
    "\n",
    "                    if video_name not in results:\n",
    "                        results[video_name] = {}\n",
    "\n",
    "                    if scene_number not in results[video_name]:\n",
    "                        results[video_name][scene_number] = {}\n",
    "\n",
    "                    if object_class not in results[video_name][scene_number]:\n",
    "                        results[video_name][scene_number][object_class] = []\n",
    "\n",
    "                    results[video_name][scene_number][object_class].append({\n",
    "                        'image': image_path,\n",
    "                        'caption': caption,\n",
    "                        'colors': colors_in_caption\n",
    "                    })\n",
    "    return results\n",
    "\n",
    "def main(input_folder, output_folder):\n",
    "    \"\"\"\n",
    "    Main function to process multiple video folders, generate captions, and extract colors.\n",
    "\n",
    "    Args:\n",
    "        input_folder (str): Path to the input folder containing video folders.\n",
    "        output_folder (str): Path to the output folder to save results.\n",
    "    \"\"\"\n",
    "    all_results = {}\n",
    "    for root, dirs, _ in os.walk(input_folder):\n",
    "        for dir_name in tqdm(dirs, desc=\"Processing directories\", unit=\"dir\"):\n",
    "            if re.match(r'\\d{5}_[ec]', dir_name):\n",
    "                video_folder = os.path.join(root, dir_name, 'crops')\n",
    "                video_results = process_video_folder(video_folder)\n",
    "                for video_name, scenes in video_results.items():\n",
    "                    if video_name not in all_results:\n",
    "                        all_results[video_name] = scenes\n",
    "                    else:\n",
    "                        for scene_number, objects in scenes.items():\n",
    "                            print(f\"Parsing {video_name} scene {scene_number} objects\")\n",
    "                            if scene_number not in all_results[video_name]:\n",
    "                                all_results[video_name][scene_number] = objects\n",
    "                            else:\n",
    "                                for object_class, details in objects.items():\n",
    "                                    if object_class not in all_results[video_name][scene_number]:\n",
    "                                        all_results[video_name][scene_number][object_class] = details\n",
    "                                    else:\n",
    "                                        all_results[video_name][scene_number][object_class].extend(details)\n",
    "\n",
    "    # Ensure the output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Save results to JSON files, one per video\n",
    "    for video_name, data in all_results.items():\n",
    "        output_file = os.path.join(output_folder, f'{video_name}_colors.json')\n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump({video_name: data}, f, indent=4)\n",
    "\n",
    "# Example usage\n",
    "input_folder = 'yolov9-main/runs/detect'\n",
    "output_folder = 'color_detection_results_blip'\n",
    "\n",
    "main(input_folder, output_folder)\n"
   ],
   "id": "ab90975d96a53539",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Dominant Color Detection with Enhanced Vibrance and Saturation\n",
    "\n",
    "This script processes an image to identify the two most dominant colors, emphasizing vibrant colors over less vibrant ones like gray and black. The key steps involved in the process are:\n",
    "\n",
    "1. **Image Preprocessing**:\n",
    "   - **Saturation Adjustment**: Increases the saturation of all colors in the image to make colors more vivid.\n",
    "   - **Vibrance Adjustment**: Boosts the vibrance of colors, particularly those that are less saturated, ensuring more nuanced color enhancement.\n",
    "\n",
    "2. **Color Filtering**:\n",
    "   - Converts the image to the HSV color space to filter out low-saturation colors, reducing the likelihood of selecting colors like gray or black as dominant colors.\n",
    "\n",
    "3. **K-Means Clustering**:\n",
    "   - Uses K-means clustering to identify the most common colors in the preprocessed image. The algorithm ensures that the top colors are distinct by checking the Euclidean distance between colors.\n",
    "\n",
    "4. **Color Preference Weighting**:\n",
    "   - Applies a weighting mechanism to prioritize vibrant colors (e.g., red, green, blue, yellow, orange) over less vibrant ones during the color matching process.\n",
    "\n",
    "5. **Output**:\n",
    "   - Ensures that the two most dominant colors are unique and outputs their names and RGB values. The original and preprocessed images are displayed side-by-side to visualize the effect of the adjustments."
   ],
   "id": "1c73ef5a57c74d1f"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "import webcolors\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# Define a dictionary of common colors with an additional preference weight for each color\n",
    "COMMON_COLORS = {\n",
    "    'red': ('#FF0000', 10),\n",
    "    'green': ('#008000', 10),\n",
    "    'blue': ('#0000FF', 10),\n",
    "    'yellow': ('#FFFF00', 10),\n",
    "    'black': ('#000000', 1),\n",
    "    'white': ('#FFFFFF', 10),\n",
    "    'orange': ('#FFA500', 8),\n",
    "    'purple': ('#800080', 1),\n",
    "    'brown': ('#A52A2A', 1),\n",
    "    'gray': ('#808080', 1),\n",
    "    'pink': ('#FFC0CB', 1)\n",
    "}\n",
    "\n",
    "def closest_color(requested_color):\n",
    "    \"\"\"\n",
    "    Find the closest color name from the COMMON_COLORS dictionary based on RGB distance.\n",
    "\n",
    "    Args:\n",
    "        requested_color (tuple): RGB tuple of the color to find the closest match for.\n",
    "\n",
    "    Returns:\n",
    "        str: Name of the closest color.\n",
    "    \"\"\"\n",
    "    min_colors = {}\n",
    "    for name, (hex_code, weight) in COMMON_COLORS.items():\n",
    "        r_c, g_c, b_c = webcolors.hex_to_rgb(hex_code)\n",
    "        rd = (r_c - requested_color[0]) ** 2\n",
    "        gd = (g_c - requested_color[1]) ** 2\n",
    "        bd = (b_c - requested_color[2]) ** 2\n",
    "        min_colors[(rd + gd + bd) / weight] = name\n",
    "    return min_colors[min(min_colors.keys())]\n",
    "\n",
    "def adjust_saturation(image, saturation_scale=2.0):\n",
    "    \"\"\"\n",
    "    Adjust the saturation of an image.\n",
    "\n",
    "    Args:\n",
    "        image (np.ndarray): Input image in RGB format.\n",
    "        saturation_scale (float): Scale factor for saturation adjustment.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Image with adjusted saturation.\n",
    "    \"\"\"\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV).astype(np.float32)\n",
    "    hsv[..., 1] *= saturation_scale\n",
    "    hsv[..., 1] = np.clip(hsv[..., 1], 0, 255)\n",
    "    return cv2.cvtColor(hsv.astype(np.uint8), cv2.COLOR_HSV2RGB)\n",
    "\n",
    "def adjust_vibrance(image, vibrance_scale=2.0):\n",
    "    \"\"\"\n",
    "    Adjust the vibrance of an image.\n",
    "\n",
    "    Args:\n",
    "        image (np.ndarray): Input image in RGB format.\n",
    "        vibrance_scale (float): Scale factor for vibrance adjustment.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Image with adjusted vibrance.\n",
    "    \"\"\"\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV).astype(np.float32)\n",
    "    saturation = hsv[..., 1]\n",
    "    mean_saturation = np.mean(saturation)\n",
    "    increase = (1 - (saturation / 255.0)) * (saturation - mean_saturation) * vibrance_scale\n",
    "    hsv[..., 1] = np.clip(saturation + increase, 0, 255)\n",
    "    return cv2.cvtColor(hsv.astype(np.uint8), cv2.COLOR_HSV2RGB)\n",
    "\n",
    "def preprocess_image(image):\n",
    "    \"\"\"\n",
    "    Preprocess the image by adjusting saturation and vibrance.\n",
    "\n",
    "    Args:\n",
    "        image (np.ndarray): Input image in RGB format.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Preprocessed image.\n",
    "    \"\"\"\n",
    "    image = adjust_saturation(image, saturation_scale=3.0)\n",
    "    image = adjust_vibrance(image, vibrance_scale=3.0)\n",
    "    return image\n",
    "\n",
    "def get_dominant_colors(image_path, k=10, top_n=2, min_distance=50, saturation_threshold=50):\n",
    "    \"\"\"\n",
    "    Get the dominant colors from an image.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the image file.\n",
    "        k (int): Number of clusters for KMeans.\n",
    "        top_n (int): Number of top dominant colors to return.\n",
    "        min_distance (float): Minimum distance between distinct colors.\n",
    "        saturation_threshold (float): Minimum saturation threshold for a pixel to be considered.\n",
    "\n",
    "    Returns:\n",
    "        list: Names of the top dominant colors.\n",
    "        list: RGB values of the top dominant colors.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        preprocessed_image = preprocess_image(image)\n",
    "        pixels = preprocessed_image.reshape((-1, 3))\n",
    "        hsv_pixels = cv2.cvtColor(pixels.reshape(-1, 1, 3).astype(np.uint8), cv2.COLOR_RGB2HSV).reshape(-1, 3)\n",
    "        pixels = pixels[hsv_pixels[:, 1] > saturation_threshold]\n",
    "\n",
    "        if pixels.shape[0] == 0:\n",
    "            raise ValueError(f\"No pixels with saturation above {saturation_threshold} in image {image_path}\")\n",
    "\n",
    "        unique_pixels = np.unique(pixels, axis=0)\n",
    "        if unique_pixels.shape[0] < k:\n",
    "            k = unique_pixels.shape[0]\n",
    "\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        kmeans.fit(pixels)\n",
    "\n",
    "        counts = np.bincount(kmeans.labels_)\n",
    "        dominant_indices = np.argsort(-counts)\n",
    "        dominant_colors = kmeans.cluster_centers_[dominant_indices]\n",
    "\n",
    "        distinct_colors = []\n",
    "        for color in dominant_colors:\n",
    "            if len(distinct_colors) == 0:\n",
    "                distinct_colors.append(color)\n",
    "            else:\n",
    "                if all(cdist([color], [distinct_color], metric='euclidean')[0][0] > min_distance for distinct_color in distinct_colors):\n",
    "                    distinct_colors.append(color)\n",
    "                if len(distinct_colors) >= top_n:\n",
    "                    break\n",
    "\n",
    "        dominant_color_names = [closest_color(color) for color in distinct_colors]\n",
    "\n",
    "        distinct_color_names = []\n",
    "        distinct_colors_filtered = []\n",
    "        for name, color in zip(dominant_color_names, distinct_colors):\n",
    "            if name not in distinct_color_names:\n",
    "                distinct_color_names.append(name)\n",
    "                distinct_colors_filtered.append(color)\n",
    "\n",
    "        if len(distinct_color_names) < top_n:\n",
    "            for color in dominant_colors[len(distinct_colors):]:\n",
    "                name = closest_color(color)\n",
    "                if name not in distinct_color_names:\n",
    "                    distinct_color_names.append(name)\n",
    "                    distinct_colors_filtered.append(color)\n",
    "                if len(distinct_color_names) >= top_n:\n",
    "                    break\n",
    "\n",
    "        return distinct_color_names[:top_n], distinct_colors_filtered[:top_n]\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {image_path}: {e}\")\n",
    "        return [], []\n",
    "\n",
    "def process_video_folder(video_folder):\n",
    "    \"\"\"\n",
    "    Process all image files in a video folder, detecting dominant colors.\n",
    "\n",
    "    Args:\n",
    "        video_folder (str): Path to the video folder containing image files.\n",
    "\n",
    "    Returns:\n",
    "        dict: Results organized by video name, scene number, and object class.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for root, dirs, files in os.walk(video_folder):\n",
    "        for file in files:\n",
    "            if file.endswith(\".jpg\") or file.endswith(\".png\"):\n",
    "                image_path = os.path.join(root, file)\n",
    "                dominant_color_names, dominant_colors = get_dominant_colors(image_path)\n",
    "\n",
    "                if dominant_color_names:  # Only process if colors were successfully detected\n",
    "                    match = re.match(r'.*/(\\d{5})_Scene-(\\d+)', image_path)\n",
    "                    if match:\n",
    "                        video_name = match.group(1)\n",
    "                        scene_number = match.group(2)\n",
    "                        object_class = os.path.basename(os.path.dirname(image_path))\n",
    "\n",
    "                        if video_name not in results:\n",
    "                            results[video_name] = {}\n",
    "\n",
    "                        if scene_number not in results[video_name]:\n",
    "                            results[video_name][scene_number] = {}\n",
    "\n",
    "                        if object_class not in results[video_name][scene_number]:\n",
    "                            results[video_name][scene_number][object_class] = []\n",
    "\n",
    "                        results[video_name][scene_number][object_class].append({\n",
    "                            'image': image_path,\n",
    "                            'dominant_colors': dominant_color_names\n",
    "                        })\n",
    "                        print(f\"Processed: {image_path}\")\n",
    "                        print(\"Dominant Colors:\", dominant_color_names)\n",
    "    return results\n",
    "\n",
    "def main(input_folder, output_folder):\n",
    "    \"\"\"\n",
    "    Main function to process multiple video folders, detect dominant colors, and save results to JSON files.\n",
    "\n",
    "    Args:\n",
    "        input_folder (str): Path to the input folder containing video folders.\n",
    "        output_folder (str): Path to the output folder to save results.\n",
    "    \"\"\"\n",
    "    all_results = {}\n",
    "    for root, dirs, _ in os.walk(input_folder):\n",
    "        for dir_name in dirs:\n",
    "            if re.match(r'\\d{5}_[ec]', dir_name):\n",
    "                video_folder = os.path.join(root, dir_name, 'crops')\n",
    "                video_results = process_video_folder(video_folder)\n",
    "                for video_name, scenes in video_results.items():\n",
    "                    if video_name not in all_results:\n",
    "                        all_results[video_name] = scenes\n",
    "                    else:\n",
    "                        for scene_number, objects in scenes.items():\n",
    "                            if scene_number not in all_results[video_name]:\n",
    "                                all_results[video_name][scene_number] = objects\n",
    "                            else:\n",
    "                                for object_class, details in objects.items():\n",
    "                                    if object_class not in all_results[video_name][scene_number]:\n",
    "                                        all_results[video_name][scene_number][object_class] = details\n",
    "                                    else:\n",
    "                                        all_results[video_name][scene_number][object_class].extend(details)\n",
    "\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    output_file_all = os.path.join(output_folder, f'all_detected_colors.json')\n",
    "    with open(output_file_all, 'w') as f:\n",
    "        json.dump(all_results, f, indent=4)\n",
    "        \n",
    "    for video_name, data in all_results.items():\n",
    "        output_file = os.path.join(output_folder, f'{video_name}_colors.json')\n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump({video_name: data}, f, indent=4)\n",
    "\n",
    "# Example usage\n",
    "input_folder = 'yolov9-main/runs/detect'\n",
    "output_folder = 'color_detection_results_dominant_color'\n",
    "\n",
    "main(input_folder, output_folder)\n",
    "\n",
    "# # Load your cropped image\n",
    "# image_path = 'yolov9-main/runs/detect/00120_e/crops/backpack/00120_Scene-36_.jpg'\n",
    "# \n",
    "# # Get the dominant colors in the image\n",
    "# dominant_color_names, dominant_colors, preprocessed_image = get_dominant_colors(image_path)\n",
    "# print(\"Dominant Color Names:\", dominant_color_names)\n",
    "# print(\"Dominant Colors RGB:\", dominant_colors)\n",
    "# \n",
    "# # Display the original image and the image with increased vibrance and saturation\n",
    "# original_image = cv2.imread(image_path)\n",
    "# original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
    "# \n",
    "# plt.figure(figsize=(12, 6))\n",
    "# \n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.imshow(original_image)\n",
    "# plt.title(\"Original Image\")\n",
    "# plt.axis('off')\n",
    "# \n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.imshow(preprocessed_image)\n",
    "# plt.title(f\"Preprocessed Image\\n(Dominant Colors: {dominant_color_names})\")\n",
    "# plt.axis('off')\n",
    "# \n",
    "# plt.show()\n"
   ],
   "id": "a57d5aee833f1a0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# OCR using EasyOCR\n",
    "\n",
    "This script utilizes OCR (Optical Character Recognition) to extract and process text from video frames. The primary functionalities and steps involved are:\n",
    "\n",
    "1. **Video Capture and OCR Initialization**:\n",
    "   - **Video Loading**: Uses `cv2.VideoCapture` to load the video.\n",
    "   - **OCR Reader Setup**: Initializes the `easyocr.Reader` for English text recognition.\n",
    "\n",
    "2. **Frame Extraction and Text Recognition**:\n",
    "   - **Frame Iteration**: Processes the video in steps, defined by `frame_step`, to capture frames at regular intervals.\n",
    "   - **Timestamp Calculation**: Computes the timestamp for each frame to log when the text appears.\n",
    "   - **Color Conversion**: Converts frames to RGB format for OCR processing.\n",
    "   - **Text Extraction**: Uses EasyOCR to extract text from the frames.\n",
    "\n",
    "3. **Text Similarity and Block Creation**:\n",
    "   - **Similarity Check**: Compares extracted text from consecutive frames using `difflib.SequenceMatcher` to determine similarity.\n",
    "   - **Block Management**: Groups similar text into blocks with start and end times. If the text changes significantly, a new block is created.\n",
    "\n",
    "4. **Results Compilation and Storage**:\n",
    "   - **Result Structuring**: Organizes extracted text and timestamps into a list of dictionaries.\n",
    "   - **JSON Output**: Saves the results to a JSON file for each video.\n",
    "\n",
    "5. **Batch Processing**:\n",
    "   - **Directory Traversal**: Walks through the input folder to find video files.\n",
    "   - **Output Management**: Ensures the output directory exists and skips processing if results already exist.\n"
   ],
   "id": "a14a917f3197e26"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import cv2\n",
    "import easyocr\n",
    "import difflib\n",
    "import json\n",
    "\n",
    "def ocr_video(video_path, output_file, similarity_threshold=0.5, frame_step=10):\n",
    "    \"\"\"\n",
    "    Perform OCR on a video and save the results to a JSON file.\n",
    "\n",
    "    Args:\n",
    "        video_path (str): Path to the input video file.\n",
    "        output_file (str): Path to the output JSON file.\n",
    "        similarity_threshold (float): Threshold for text similarity to merge frames into a single block.\n",
    "        frame_step (int): Number of frames to skip between each OCR operation.\n",
    "\n",
    "    Returns:\n",
    "        list: List of OCR results with text, start time, and end time.\n",
    "    \"\"\"\n",
    "    # Initialize the video capture and OCR reader\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    reader = easyocr.Reader(['en'])\n",
    "\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    # Initialize variables to store the results\n",
    "    results = []\n",
    "\n",
    "    prev_text = \"\"\n",
    "    current_block = None\n",
    "\n",
    "    for i in range(0, frame_count, frame_step):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Skip frames until we reach the next frame of interest\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "        \n",
    "        # Calculate the timestamp for the current frame\n",
    "        timestamp = i / fps\n",
    "\n",
    "        # Convert the frame to RGB (easyocr works on RGB images)\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Perform OCR on the frame\n",
    "        ocr_result_list = reader.readtext(frame_rgb, detail=0)\n",
    "        ocr_result = ' '.join(ocr_result_list).strip().lower()\n",
    "\n",
    "        # Skip frames with no text\n",
    "        if not ocr_result:\n",
    "            continue\n",
    "\n",
    "        # Calculate similarity with the previous text\n",
    "        similarity = difflib.SequenceMatcher(None, prev_text, ocr_result).ratio()\n",
    "\n",
    "        if similarity >= similarity_threshold:\n",
    "            # If the text is similar enough, update the end time of the current block\n",
    "            if current_block:\n",
    "                current_block['end_time'] = timestamp + (frame_step / fps)\n",
    "            else:\n",
    "                current_block = {\n",
    "                    'text': ocr_result,\n",
    "                    'start_time': timestamp,\n",
    "                    'end_time': timestamp + (frame_step / fps)\n",
    "                }\n",
    "        else:\n",
    "            # If the text is different enough, finalize the current block and start a new one\n",
    "            if current_block:\n",
    "                results.append(current_block)\n",
    "            current_block = {\n",
    "                'text': ocr_result,\n",
    "                'start_time': timestamp,\n",
    "                'end_time': timestamp + (frame_step / fps)\n",
    "            }\n",
    "        prev_text = ocr_result\n",
    "\n",
    "    # Finalize the last block\n",
    "    if current_block:\n",
    "        results.append(current_block)\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    # Save results to a JSON file\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "\n",
    "    return results\n",
    "\n",
    "def process_videos(input_folder, output_folder, similarity_threshold=0.5, frame_step=10):\n",
    "    \"\"\"\n",
    "    Process multiple videos in a folder, perform OCR, and save the results to JSON files.\n",
    "\n",
    "    Args:\n",
    "        input_folder (str): Path to the folder containing input videos.\n",
    "        output_folder (str): Path to the folder to save the OCR results.\n",
    "        similarity_threshold (float): Threshold for text similarity to merge frames into a single block.\n",
    "        frame_step (int): Number of frames to skip between each OCR operation.\n",
    "    \"\"\"\n",
    "    # Ensure the output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Traverse the input folder\n",
    "    for root, dirs, files in os.walk(input_folder):\n",
    "        for dir_name in dirs:\n",
    "            video_folder = os.path.join(root, dir_name)\n",
    "            video_file = os.path.join(video_folder, f\"{dir_name}.mp4\")\n",
    "            if os.path.exists(video_file):\n",
    "                output_file = os.path.join(output_folder, f\"{dir_name}.json\")\n",
    "                # Check if the output file already exists\n",
    "                if os.path.exists(output_file):\n",
    "                    print(f\"Skipping {video_file} as output already exists.\")\n",
    "                    continue\n",
    "                print(f\"Processing video: {video_file}\")\n",
    "                ocr_video(video_file, output_file, similarity_threshold, frame_step)\n",
    "\n",
    "# Example usage\n",
    "input_folder = 'preprocessed_videos'\n",
    "output_folder = 'ocr_results'\n",
    "similarity_threshold = 0.5\n",
    "frame_step = 10\n",
    "\n",
    "process_videos(input_folder, output_folder, similarity_threshold, frame_step)\n"
   ],
   "id": "59bca590da301029",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Search for a string in OCR results\n",
    "\n",
    "This script compares an input string against OCR results stored in a JSON file to identify similar text segments. The key steps and functionalities are:\n",
    "\n",
    "1. **Loading OCR Results**:\n",
    "   - **File Reading**: Loads OCR results from a specified JSON file. Each OCR result contains text and its corresponding start and end timestamps.\n",
    "\n",
    "2. **String Normalization**:\n",
    "   - **Input String Processing**: Strips and converts the input string to lowercase for case-insensitive comparison.\n",
    "\n",
    "3. **Similarity Comparison**:\n",
    "   - **Text Comparison**: Uses `difflib.SequenceMatcher` to calculate the similarity ratio between the input string and each OCR result's text.\n",
    "   - **Threshold Filtering**: Compares the similarity ratio against a specified threshold to determine if the texts are similar enough.\n",
    "\n",
    "4. **Result Compilation**:\n",
    "   - **Matching Results**: Collects OCR results that meet the similarity threshold, including their text and timestamps.\n"
   ],
   "id": "73b6a69390285bd2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "import difflib\n",
    "\n",
    "def compare_string_with_ocr_results(ocr_file, input_string, similarity_threshold=0.8):\n",
    "    \"\"\"\n",
    "    Compare an input string with OCR results from a JSON file and return matching results based on similarity.\n",
    "\n",
    "    Args:\n",
    "        ocr_file (str): Path to the JSON file containing OCR results.\n",
    "        input_string (str): The input string to compare with the OCR results.\n",
    "        similarity_threshold (float): The minimum similarity ratio to consider a match (default is 0.8).\n",
    "\n",
    "    Returns:\n",
    "        list: A list of matching results, each containing the matching text, start time, and end time.\n",
    "    \"\"\"\n",
    "    # Load OCR results from the file\n",
    "    with open(ocr_file, 'r') as f:\n",
    "        ocr_results = json.load(f)\n",
    "\n",
    "    # Normalize the input string\n",
    "    input_string = input_string.strip().lower()\n",
    "\n",
    "    matching_results = []\n",
    "\n",
    "    for result in ocr_results:\n",
    "        ocr_text = result['text']\n",
    "        similarity = difflib.SequenceMatcher(None, ocr_text, input_string).ratio()\n",
    "\n",
    "        if similarity >= similarity_threshold:\n",
    "            matching_results.append({\n",
    "                'text': ocr_text,\n",
    "                'start_time': result['start_time'],\n",
    "                'end_time': result['end_time']\n",
    "            })\n",
    "\n",
    "    return matching_results\n",
    "\n",
    "# Example usage\n",
    "ocr_file = 'ocr_results.json'\n",
    "input_string = 'Get reliable diving gear'\n",
    "similarity_threshold = 0.6\n",
    "\n",
    "matching_results = compare_string_with_ocr_results(ocr_file, input_string, similarity_threshold)\n",
    "\n",
    "for result in matching_results:\n",
    "    print(f\"Matching Text: {result['text']}, Start Time: {result['start_time']:.2f}, End Time: {result['end_time']:.2f}\")\n"
   ],
   "id": "b14682c29ba71b28",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
